{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5879c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve, auc, mean_squared_error, \\\n",
    "    r2_score, mean_absolute_error,cohen_kappa_score,accuracy_score,f1_score,matthews_corrcoef,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def standardize(col):\n",
    "    return (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "\n",
    "# the metrics for classification\n",
    "def statistical(y_true, y_pred, y_pro):\n",
    "    c_mat = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = list(c_mat.flatten())\n",
    "    se = tp / (tp + fn)\n",
    "    sp = tn / (tn + fp)\n",
    "    auc_prc = auc(precision_recall_curve(y_true, y_pro, pos_label=1)[1],\n",
    "                  precision_recall_curve(y_true, y_pro, pos_label=1)[0])\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "#     acc_skl = accuracy_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pro)\n",
    "    recall = se\n",
    "#     recall_skl = recall_score(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "#     precision_skl = precision_score(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     f1_skl = f1_score(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n",
    "#     mcc_skl = matthews_corrcoef(y_true,y_pred)\n",
    "    return tn,fp,fn,tp,se,sp,auc_prc,acc,auc_roc,recall,precision,f1,kappa,mcc\n",
    "\n",
    "def all_one_zeros(data):\n",
    "    if (len(np.unique(data)) == 2):\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "feature_selection = False\n",
    "tasks_dic = {'0-CF-2274-desc-split.csv': ['activity']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6daf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '0-CF-2274-desc-split.csv'\n",
    "task_type = 'cla'  # 'reg' or 'cla'\n",
    "dataset_label = file_name.split('/')[-1].split('_')[0]\n",
    "tasks = tasks_dic[dataset_label]\n",
    "OPT_ITERS = 50\n",
    "repetitions = 50\n",
    "num_pools = 5\n",
    "unbalance = True\n",
    "patience = 100\n",
    "space_ = {'num_leaves': hp.choice('num_leaves', range(100, 300,10)),\n",
    "          'learning_rate': hp.uniform('learning_rate', 0.005, 0.3),\n",
    "          'max_depth': hp.choice('max_depth', range(3, 12)),\n",
    "          'n_estimators': hp.choice('n_estimators', [100,200,300, 400, 500, 1000]),\n",
    "          'min_child_samples': hp.choice('min_child_samples', range(0, 100,10)),\n",
    "          'max_bin': hp.choice('max_bin', range(300, 400,10))\n",
    "          }\n",
    "num_leaves_ls = range(100, 300,10)\n",
    "max_depth_ls = range(3, 12)\n",
    "n_estimators_ls = [100,200,300, 400, 500, 1000]\n",
    "min_child_samples_ls = range(0, 100,10)\n",
    "max_bin_ls = range(300, 400,10)\n",
    "dataset = pd.read_csv(file_name)\n",
    "pd_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5508043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_runing(subtask):\n",
    "    cols = [subtask]\n",
    "    cols.extend(dataset.columns[(len(tasks) + 1):])\n",
    "    sub_dataset = dataset[cols]\n",
    "\n",
    "    # detect the na in the subtask (y cloumn)\n",
    "    rm_index = sub_dataset[subtask][sub_dataset[subtask].isnull()].index\n",
    "    sub_dataset.drop(index=rm_index, inplace=True)\n",
    "\n",
    "    # remove the features with na\n",
    "    if dataset_label != 'hiv':\n",
    "        sub_dataset = sub_dataset.dropna(axis=1)\n",
    "    else:\n",
    "        sub_dataset = sub_dataset.dropna(axis=0)\n",
    "    # *******************\n",
    "    # demension reduction\n",
    "    # *******************\n",
    "    # Removing features with low variance\n",
    "    # threshold = 0.05\n",
    "    data_fea_var = sub_dataset.iloc[:, 2:].var()\n",
    "    del_fea1 = list(data_fea_var[data_fea_var <= 0.05].index)\n",
    "    sub_dataset.drop(columns=del_fea1, inplace=True)\n",
    "\n",
    "    # pair correlations\n",
    "    # threshold = 0.95\n",
    "    data_fea_corr = sub_dataset.iloc[:, 2:].corr()\n",
    "    del_fea2_col = []\n",
    "    del_fea2_ind = []\n",
    "    length = data_fea_corr.shape[1]\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if abs(data_fea_corr.iloc[i, j]) >= 0.95:\n",
    "                del_fea2_col.append(data_fea_corr.columns[i])\n",
    "                del_fea2_ind.append(data_fea_corr.index[j])\n",
    "    sub_dataset.drop(columns=del_fea2_ind, inplace=True)\n",
    "\n",
    "    # standardize the features\n",
    "    cols_ = list(sub_dataset.columns)[2:]\n",
    "    sub_dataset[cols_] = sub_dataset[cols_].apply(standardize, axis=0)\n",
    "\n",
    "    # get the attentivefp data splits\n",
    "    data_tr = sub_dataset[sub_dataset['group'] == 'train']\n",
    "    data_va = sub_dataset[sub_dataset['group'] == 'valid']\n",
    "    data_te = sub_dataset[sub_dataset['group'] == 'test']\n",
    "\n",
    "    # prepare data for training\n",
    "    # training set\n",
    "    data_tr_y = data_tr[subtask].values.reshape(-1, 1)\n",
    "    data_tr_x = np.array(data_tr.iloc[:, 2:].values)\n",
    "\n",
    "    # validation set\n",
    "    data_va_y = data_va[subtask].values.reshape(-1, 1)\n",
    "    data_va_x = np.array(data_va.iloc[:, 2:].values)\n",
    "\n",
    "    # test set\n",
    "    data_te_y = data_te[subtask].values.reshape(-1, 1)\n",
    "    data_te_x = np.array(data_te.iloc[:, 2:].values)\n",
    "\n",
    "    if feature_selection:\n",
    "        # univariate feature selection\n",
    "        trans1 = SelectPercentile(f_classif, percentile=80)\n",
    "        trans1.fit(data_tr_x, data_tr_y)\n",
    "        data_tr_x = trans1.transform(data_tr_x)\n",
    "        data_va_x = trans1.transform(data_va_x)\n",
    "        data_te_x = trans1.transform(data_te_x)\n",
    "\n",
    "        # select from model\n",
    "        clf = XGBClassifier(n_jobs=12, random_state=1)\n",
    "        clf = clf.fit(data_tr_x, data_tr_y)\n",
    "        trans2 = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        data_tr_x = trans2.transform(data_tr_x)\n",
    "        data_va_x = trans2.transform(data_va_x)\n",
    "        data_te_x = trans2.transform(data_te_x)\n",
    "\n",
    "    num_fea = data_tr_x.shape[1]\n",
    "    print('the num of retained features for the ' + dataset_label + ' ' + subtask + ' is:', num_fea)\n",
    "\n",
    "    def hyper_opt(args):\n",
    "        model = LGBMClassifier(**args, n_jobs=6, random_state=1,\n",
    "                              is_unbalance = unbalance) if task_type == 'cla' else LGBMRegressor(**args, n_jobs=6,\n",
    "                                                                                                   random_state=1)\n",
    "\n",
    "        model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "                  eval_set=[(data_va_x, data_va_y)],\n",
    "                  early_stopping_rounds=patience, verbose=False)\n",
    "        val_preds = model.predict_proba(data_va_x) if task_type == 'cla' else \\\n",
    "            model.predict(data_va_x)\n",
    "        loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1]) if task_type == 'cla' else np.sqrt(\n",
    "            mean_squared_error(data_va_y, val_preds))\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    # start hyper-parameters optimization\n",
    "    trials = Trials()\n",
    "    best_results = fmin(hyper_opt, space_, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials, show_progressbar=False)\n",
    "    print('the best hyper-parameters for ' + dataset_label + ' ' + subtask + ' are:  ', best_results)\n",
    "    best_model = LGBMClassifier(num_leaves=num_leaves_ls[best_results['num_leaves']],\n",
    "                                        learning_rate=best_results['learning_rate'],\n",
    "                                        max_depth=max_depth_ls[best_results['max_depth']],\n",
    "                                        n_estimators=n_estimators_ls[best_results['n_estimators']],\n",
    "                                        min_child_samples=min_child_samples_ls[best_results['min_child_samples']],\n",
    "                                        max_bin=max_bin_ls[best_results['max_bin']],\n",
    "                                        n_jobs=6, random_state=1, verbose=-1, is_unbalance = unbalance) \\\n",
    "        if task_type == 'cla' else LGBMRegressor(\n",
    "        num_leaves=num_leaves_ls[best_results['num_leaves']],\n",
    "        learning_rate=best_results['learning_rate'],\n",
    "        max_depth=max_depth_ls[best_results['max_depth']],\n",
    "        n_estimators=n_estimators_ls[best_results['n_estimators']],\n",
    "        min_child_samples=min_child_samples_ls[best_results['min_child_samples']],\n",
    "        max_bin=max_bin_ls[best_results['max_bin']],\n",
    "        n_jobs=6, random_state=1, verbose=-1)  \n",
    "    \n",
    "    best_model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "                   eval_set=[(data_va_x, data_va_y)],\n",
    "                   early_stopping_rounds=patience, verbose=False)\n",
    "    num_of_compounds = len(sub_dataset)\n",
    "\n",
    "    if task_type == 'cla':\n",
    "        # training set\n",
    "        tr_pred = best_model.predict_proba(data_tr_x)\n",
    "        tr_results = [dataset_label, subtask, 'tr', num_fea, num_of_compounds, data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0] / data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']]]\n",
    "        tr_results.extend(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n",
    "        # validation set\n",
    "        va_pred = best_model.predict_proba(data_va_x)\n",
    "                      \n",
    "        va_results = [dataset_label, subtask, 'va', num_fea, num_of_compounds, data_va_y[data_va_y == 1].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0] / data_va_y[data_va_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']]]\n",
    "        va_results.extend(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n",
    "\n",
    "        # test set\n",
    "        te_pred = best_model.predict_proba(data_te_x)\n",
    "        te_results = [dataset_label, subtask, 'te', num_fea, num_of_compounds, data_te_y[data_te_y == 1].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0] / data_te_y[data_te_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']]]\n",
    "        te_results.extend(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n",
    "    else:\n",
    "        # training set\n",
    "        tr_pred = best_model.predict(data_tr_x)\n",
    "        tr_results = [dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']],\n",
    "                      np.sqrt(mean_squared_error(data_tr_y, tr_pred)), r2_score(data_tr_y, tr_pred),\n",
    "                      mean_absolute_error(data_tr_y, tr_pred)]\n",
    "\n",
    "        # validation set\n",
    "        va_pred = best_model.predict(data_va_x)\n",
    "        va_results = [dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']],\n",
    "                      np.sqrt(mean_squared_error(data_va_y, va_pred)), r2_score(data_va_y, va_pred),\n",
    "                      mean_absolute_error(data_va_y, va_pred)]\n",
    "\n",
    "        # test set\n",
    "        te_pred = best_model.predict(data_te_x)\n",
    "        te_results = [dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']],\n",
    "                      np.sqrt(mean_squared_error(data_te_y, te_pred)), r2_score(data_te_y, te_pred),\n",
    "                      mean_absolute_error(data_te_y, te_pred)]\n",
    "    return tr_results, va_results, te_results\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(num_pools)\n",
    "res = pool.starmap(hyper_runing, zip(tasks))\n",
    "pool.close()\n",
    "pool.join()\n",
    "for item in res:\n",
    "    for i in range(3):\n",
    "        pd_res.append(item[i])\n",
    "if task_type == 'cla':\n",
    "    best_hyper = pd.DataFrame(pd_res, columns=['dataset', 'subtask', 'set',\n",
    "                                               'num_of_retained_feature',\n",
    "                                               'num_of_compounds', 'postives',\n",
    "                                               'negtives', 'negtives/postives',\n",
    "                                               'num_leaves', \n",
    "                                               'learning_rate','max_depth','n_estimators','min_child_samples','max_bin',\n",
    "                                               'tn', 'fp', 'fn', 'tp', 'se', 'sp',\n",
    "                                               'auc_prc', 'acc', 'auc_roc','recall','precision','f1','kappa','mcc'])\n",
    "else:\n",
    "    best_hyper = pd.DataFrame(pd_res, columns=['dataset', 'subtask', 'set',\n",
    "                                               'num_leaves', \n",
    "                                               'learning_rate','max_depth','n_estimators','min_child_samples','max_bin',\n",
    "                                               'rmse', 'r2', 'mae'])\n",
    "best_hyper.to_csv('./stat_res/' + dataset_label + '_moe_pubsub_LGB_hyperopt_info.csv', index=0)\n",
    "\n",
    "if task_type == 'cla':\n",
    "    print('train', best_hyper[best_hyper['set'] == 'tr']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'tr']['auc_prc'].mean())\n",
    "    print('valid', best_hyper[best_hyper['set'] == 'va']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'va']['auc_prc'].mean())\n",
    "    print('test', best_hyper[best_hyper['set'] == 'te']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'te']['auc_prc'].mean())\n",
    "else:\n",
    "    print('train', best_hyper[best_hyper['set'] == 'tr']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'tr']['r2'].mean(), best_hyper[best_hyper['set'] == 'tr']['mae'].mean())\n",
    "    print('valid', best_hyper[best_hyper['set'] == 'va']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'va']['r2'].mean(), best_hyper[best_hyper['set'] == 'va']['mae'].mean())\n",
    "    print('test', best_hyper[best_hyper['set'] == 'te']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'te']['r2'].mean(), best_hyper[best_hyper['set'] == 'te']['mae'].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 repetitions based on thr best hypers\n",
    "dataset.drop(columns=['group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "49892864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed used in repetition 13 is 13\n",
      "random seed used in repetition 1 is 1\n",
      "random seed used in repetition 10 is 10\n",
      "random seed used in repetition 4 is 4\n",
      "random seed used in repetition 7 is 7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 5 is 5\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 14 is 14\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 8 is 8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 11 is 11\n",
      "random seed used in repetition 2 is 2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 15 is 15\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 6 is 6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 9 is 9\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 16 is 16\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 19 is 19\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 3 is 3\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 12 is 12\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 17 is 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 22 is 22\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 20 is 20\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 25 is 25\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 18 is 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 28 is 28\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 21 is 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 23 is 23\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 31 is 31\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 26 is 26\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 34 is 34\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 29 is 29\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 24 is 24\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 27 is 27\n",
      "random seed used in repetition 32 is 32\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 35 is 35\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 30 is 30\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 37 is 37\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 33 is 33\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 40 is 40\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "random seed used in repetition 36 is 36\n",
      "random seed used in repetition 43 is 43\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 46 is 46\n",
      "random seed used in repetition 38 is 38\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 49 is 49\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 44 is 44\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 41 is 41\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 47 is 47\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 50 is 50\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 45 is 45\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 39 is 39\n",
      "random seed used in repetition 42 is 42\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 48 is 48\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "0-CF-2274-desc-split.csv_XGB: the mean auc_roc for the training set is 0.991 with std 0.015\n",
      "0-CF-2274-desc-split.csv_XGB: the mean auc_roc for the validation set is 0.878 with std 0.030\n",
      "0-CF-2274-desc-split.csv_XGB: the mean auc_roc for the test set is 0.864 with std 0.031\n",
      "the total elapsed time is: 2649.787378549576 S\n"
     ]
    }
   ],
   "source": [
    "pd_res = []\n",
    "def best_model_runing(split):\n",
    "    seed = split\n",
    "    if task_type == 'cla':\n",
    "        while True:\n",
    "            training_data, data_te = train_test_split(sub_dataset, test_size=0.1, random_state=seed)\n",
    "            # the training set was further splited into the training set and validation set\n",
    "            data_tr, data_va = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "            if (all_one_zeros(data_tr[subtask]) or all_one_zeros(data_va[subtask]) or all_one_zeros(data_te[subtask])):\n",
    "                print(\n",
    "                    '\\ninvalid random seed {} due to one class presented in the {} splitted sets...'.format(seed,\n",
    "                                                                                                            subtask))\n",
    "                print('Changing to another random seed...\\n')\n",
    "                seed = np.random.randint(50, 999999)\n",
    "            else:\n",
    "                print('random seed used in repetition {} is {}'.format(split, seed))\n",
    "                break\n",
    "    else:\n",
    "        training_data, data_te = train_test_split(sub_dataset, test_size=0.1, random_state=seed)\n",
    "        # the training set was further splited into the training set and validation set\n",
    "        data_tr, data_va = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "\n",
    "    # prepare data for training\n",
    "    # training set\n",
    "    data_tr_y = data_tr[subtask].values.reshape(-1, 1)\n",
    "    data_tr_x = np.array(data_tr.iloc[:, 1:].values)\n",
    "\n",
    "    # validation set\n",
    "    data_va_y = data_va[subtask].values.reshape(-1, 1)\n",
    "    data_va_x = np.array(data_va.iloc[:, 1:].values)\n",
    "\n",
    "    # test set\n",
    "    data_te_y = data_te[subtask].values.reshape(-1, 1)\n",
    "    data_te_x = np.array(data_te.iloc[:, 1:].values)\n",
    "\n",
    "    if feature_selection:\n",
    "        # univariate feature selection\n",
    "        trans1 = SelectPercentile(f_classif, percentile=80)\n",
    "        trans1.fit(data_tr_x, data_tr_y)\n",
    "        data_tr_x = trans1.transform(data_tr_x)\n",
    "        data_va_x = trans1.transform(data_va_x)\n",
    "        data_te_x = trans1.transform(data_te_x)\n",
    "\n",
    "        # select from model\n",
    "        clf = XGBClassifier(n_jobs=6, random_state=1)\n",
    "        clf = clf.fit(data_tr_x, data_tr_y)\n",
    "        trans2 = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        data_tr_x = trans2.transform(data_tr_x)\n",
    "        data_va_x = trans2.transform(data_va_x)\n",
    "        data_te_x = trans2.transform(data_te_x)\n",
    "\n",
    "    num_fea = data_tr_x.shape[1]\n",
    "    pos_weight = (len(sub_dataset) - sum(sub_dataset[subtask])) / sum(sub_dataset[subtask])\n",
    "#     model = LGBMClassifier(num_leaves=best_hyper[best_hyper.subtask == subtask].iloc[0,]['num_leaves'],\n",
    "#                           learning_rate=best_hyper[best_hyper.subtask == subtask].iloc[0,]['learning_rate'],\n",
    "#                           max_depth=best_hyper[best_hyper.subtask == subtask].iloc[0,]['max_depth'],\n",
    "#                           n_estimators=best_hyper[best_hyper.subtask == subtask].iloc[0,]['n_estimators'],\n",
    "#                           min_child_samples=best_hyper[best_hyper.subtask == subtask].iloc[0,]['min_child_samples'],\n",
    "#                           n_jobs=6, random_state=1,is_unbalance = unbalance) \\\n",
    "\n",
    "# Set the hyper-parameters based on previous experience .Use the above comment code if you need to tune hyper parameters\n",
    "    model =LGBMClassifier(\n",
    "                        boosting_type = 'gbdt',\n",
    "                        class_weight = None,\n",
    "                        colsample_bytree = 1.0,\n",
    "                        importance_type = 'split',\n",
    "                        learning_rate  = 0.1219922716918981,\n",
    "                        max_depth  = 15,\n",
    "                        min_child_samples  = 58,\n",
    "                        min_child_weight  = 0.001,\n",
    "                        min_split_gain  = 0.0,\n",
    "                        n_estimators  = 300,\n",
    "                        num_leaves  = 189,\n",
    "                        objective  = None,\n",
    "                        reg_alpha  = 8.031751803464846e-06,\n",
    "                        reg_lambda  = 0.0649255805574003,\n",
    "                        silent  = True,\n",
    "                        subsample  = 1.0,\n",
    "                        subsample_for_bin  = 200000,\n",
    "                        subsample_freq  = 0,\n",
    "                        bagging_fraction  = 1.0,\n",
    "                        bagging_freq  = 2,\n",
    "                        feature_fraction  = 0.7344966859224016,\n",
    "                          n_jobs=6, random_state=1,\n",
    "                        ) \\\n",
    "        if task_type == 'cla' else LGBMRegressor(\n",
    "        num_leaves=best_hyper[best_hyper.subtask == subtask].iloc[0,]['num_leaves'],\n",
    "        learning_rate=best_hyper[best_hyper.subtask == subtask].iloc[0,]['learning_rate'],\n",
    "        max_depth=best_hyper[best_hyper.subtask == subtask].iloc[0,]['max_depth'],\n",
    "        n_estimators=best_hyper[best_hyper.subtask == subtask].iloc[0,]['n_estimators'],\n",
    "        min_child_samples=best_hyper[best_hyper.subtask == subtask].iloc[0,]['min_child_samples'],\n",
    "        n_jobs=6, random_state=1, seed=1)\n",
    "\n",
    "    model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "              eval_set=[(data_va_x, data_va_y)],\n",
    "              early_stopping_rounds=patience, verbose=False)\n",
    "    num_of_compounds = sub_dataset.shape[0]\n",
    "    if task_type == 'cla':\n",
    "        # training set\n",
    "        tr_pred = model.predict_proba(data_tr_x)\n",
    "        tr_results = [split, dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0] / data_tr_y[data_tr_y == 1].shape[0]]\n",
    "        tr_results.extend(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n",
    "\n",
    "        # validation set\n",
    "        va_pred = model.predict_proba(data_va_x)\n",
    "        va_results = [split, dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      data_va_y[data_va_y == 1].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0] / data_va_y[data_va_y == 1].shape[0]]\n",
    "        va_results.extend(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n",
    "\n",
    "        # test set\n",
    "        te_pred = model.predict_proba(data_te_x)\n",
    "        te_results = [split, dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      data_te_y[data_te_y == 1].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0] / data_te_y[data_te_y == 1].shape[0]]\n",
    "        te_results.extend(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n",
    "    else:\n",
    "        # training set\n",
    "        tr_pred = model.predict(data_tr_x)\n",
    "        tr_results = [split, dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_tr_y, tr_pred)), r2_score(data_tr_y, tr_pred),\n",
    "                      mean_absolute_error(data_tr_y, tr_pred)]\n",
    "\n",
    "        # validation set\n",
    "        va_pred = model.predict(data_va_x)\n",
    "        va_results = [split, dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_va_y, va_pred)), r2_score(data_va_y, va_pred),\n",
    "                      mean_absolute_error(data_va_y, va_pred)]\n",
    "\n",
    "        # test set\n",
    "        te_pred = model.predict(data_te_x)\n",
    "        te_results = [split, dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_te_y, te_pred)), r2_score(data_te_y, te_pred),\n",
    "                      mean_absolute_error(data_te_y, te_pred)]\n",
    "    return tr_results, va_results, te_results\n",
    "\n",
    "\n",
    "for subtask in tasks:\n",
    "    cols = [subtask]\n",
    "    cols.extend(dataset.columns[(len(tasks) + 1):])\n",
    "    # cols.extend(dataset.columns[(617+1):])\n",
    "    sub_dataset = dataset[cols]\n",
    "\n",
    "    # detect the NA in the subtask (y cloumn)\n",
    "    rm_index = sub_dataset[subtask][sub_dataset[subtask].isnull()].index\n",
    "    sub_dataset.drop(index=rm_index, inplace=True)\n",
    "\n",
    "    # remove the features with na\n",
    "    if dataset_label != 'hiv':\n",
    "        sub_dataset = sub_dataset.dropna(axis=1)\n",
    "    else:\n",
    "        sub_dataset = sub_dataset.dropna(axis=0)\n",
    "\n",
    "    # *******************\n",
    "    # demension reduction\n",
    "    # *******************\n",
    "    # Removing features with low variance\n",
    "    # threshold = 0.05\n",
    "    data_fea_var = sub_dataset.iloc[:, 1:].var()\n",
    "    del_fea1 = list(data_fea_var[data_fea_var <= 0.05].index)\n",
    "    sub_dataset.drop(columns=del_fea1, inplace=True)\n",
    "\n",
    "    # pair correlations\n",
    "    # threshold = 0.95\n",
    "    data_fea_corr = sub_dataset.iloc[:, 1:].corr()\n",
    "    del_fea2_col = []\n",
    "    del_fea2_ind = []\n",
    "    length = data_fea_corr.shape[1]\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if abs(data_fea_corr.iloc[i, j]) >= 0.95:\n",
    "                del_fea2_col.append(data_fea_corr.columns[i])\n",
    "                del_fea2_ind.append(data_fea_corr.index[j])\n",
    "    sub_dataset.drop(columns=del_fea2_ind, inplace=True)\n",
    "\n",
    "    # standardize the features\n",
    "    cols_ = list(sub_dataset.columns)[1:]\n",
    "    sub_dataset[cols_] = sub_dataset[cols_].apply(standardize, axis=0)\n",
    "\n",
    "    # for split in range(1, splits+1):\n",
    "    pool = multiprocessing.Pool(num_pools)\n",
    "    res = pool.starmap(best_model_runing, zip(range(1, repetitions + 1)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for item in res:\n",
    "        for i in range(3):\n",
    "            pd_res.append(item[i])\n",
    "if task_type == 'cla':\n",
    "    stat_res = pd.DataFrame(pd_res, columns=['split', 'dataset', 'subtask', 'set',\n",
    "                                             'num_of_retained_feature',\n",
    "                                             'num_of_compounds', 'postives',\n",
    "                                             'negtives', 'negtives/postives',\n",
    "                                             'tn', 'fp', 'fn', 'tp', 'se', 'sp',\n",
    "                                             'auc_prc', 'acc', 'auc_roc','recall','precision','f1','kappa','mcc'])\n",
    "else:\n",
    "    stat_res = pd.DataFrame(pd_res, columns=['split', 'dataset', 'subtask', 'set',\n",
    "                                             'num_of_retained_feature',\n",
    "                                             'num_of_compounds', 'rmse', 'r2', 'mae'])\n",
    "stat_res.to_csv('./stat_res/' + dataset_label + '_xgb_statistical_results_split50_20220820.csv', index=0)\n",
    "# single tasks\n",
    "if len(tasks) == 1:\n",
    "    args = {'data_label': dataset_label, 'metric': 'auc_roc' if task_type == 'cla' else 'rmse', 'model': 'XGB'}\n",
    "    print('{}_{}: the mean {} for the training set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(\n",
    "            stat_res[stat_res['set'] == 'tr'][args['metric']]), np.std(\n",
    "            stat_res[stat_res['set'] == 'tr'][args['metric']])))\n",
    "    print(\n",
    "        '{}_{}: the mean {} for the validation set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(\n",
    "                stat_res[stat_res['set'] == 'va'][args['metric']]), np.std(\n",
    "                stat_res[stat_res['set'] == 'va'][args['metric']])))\n",
    "    print('{}_{}: the mean {} for the test set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                 args['metric'], np.mean(\n",
    "            stat_res[stat_res['set'] == 'te'][args['metric']]), np.std(\n",
    "            stat_res[stat_res['set'] == 'te'][args['metric']])))\n",
    "# multi-tasks\n",
    "else:\n",
    "    args = {'data_label': dataset_label, 'metric': 'auc_roc' if dataset_label != 'muv' else 'auc_prc', 'model': 'LGB'}\n",
    "    tr_acc = np.zeros(repetitions)\n",
    "    va_acc = np.zeros(repetitions)\n",
    "    te_acc = np.zeros(repetitions)\n",
    "    for subtask in tasks:\n",
    "        tr = stat_res[stat_res['set'] == 'tr']\n",
    "        tr_acc = tr_acc + tr[tr['subtask'] == subtask][args['metric']].values\n",
    "\n",
    "        va = stat_res[stat_res['set'] == 'va']\n",
    "        va_acc = va_acc + va[va['subtask'] == subtask][args['metric']].values\n",
    "\n",
    "        te = stat_res[stat_res['set'] == 'te']\n",
    "        te_acc = te_acc + te[te['subtask'] == subtask][args['metric']].values\n",
    "    tr_acc = tr_acc / len(tasks)\n",
    "    va_acc = va_acc / len(tasks)\n",
    "    te_acc = te_acc / len(tasks)\n",
    "    print('{}_{}: the mean {} for the training set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(tr_acc),\n",
    "                                                                                     np.std(tr_acc)))\n",
    "    print(\n",
    "        '{}_{}: the mean {} for the validation set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(va_acc),\n",
    "                                                                                     np.std(va_acc)))\n",
    "    print('{}_{}: the mean {} for the test set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                 args['metric'], np.mean(te_acc),\n",
    "                                                                                 np.std(te_acc)))\n",
    "end = time.time()  # get the end time\n",
    "print('the total elapsed time is:', (end - start), 'S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "50bc4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the elapsed time is: 0.7360520495971044 H\n"
     ]
    }
   ],
   "source": [
    "# acc auc_roc recall precision f1 kappa mcc\n",
    "acc_str = 'acc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['acc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['acc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['acc']),\n",
    ")\n",
    "auc_str = 'auc_roc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['auc_roc']),\n",
    ")\n",
    "recall_str = 'recall of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['recall']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['recall']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['recall']),\n",
    ")\n",
    "precision_str = 'precision of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['precision']),\n",
    ")\n",
    "f1_str = 'f1 of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['f1']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['f1']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['f1']),\n",
    ")\n",
    "kappa_str = 'kappa of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['kappa']),\n",
    ")\n",
    "mcc_str = 'mcc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['mcc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['mcc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['mcc']),\n",
    ")\n",
    "print('the elapsed time is:', (end - start)/3600, 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bfb4f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of training set is 0.964±0.036, validation set is 0.874±0.023, test set is 0.867±0.023\n",
      "auc_roc of training set is 0.991±0.015, validation set is 0.878±0.030, test set is 0.864±0.031\n",
      "recall of training set is 0.821±0.193, validation set is 0.440±0.089, test set is 0.429±0.102\n",
      "precision of training set is 0.976±0.020, validation set is 0.757±0.099, test set is 0.739±0.112\n",
      "f1 of training set is 0.877±0.159, validation set is 0.549±0.089, test set is 0.532±0.106\n",
      "kappa of training set is 0.859±0.170, validation set is 0.483±0.091, test set is 0.462±0.105\n",
      "mcc of training set is 0.869±0.149, validation set is 0.510±0.082, test set is 0.489±0.097\n"
     ]
    }
   ],
   "source": [
    "print(acc_str)\n",
    "print(auc_str)\n",
    "print(recall_str)\n",
    "print(precision_str)\n",
    "print(f1_str)\n",
    "print(kappa_str)\n",
    "print(mcc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ee4ebafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_lgb_20220906.txt', 'w') as f:\n",
    "    f.write(acc_str+'\\n')\n",
    "    f.write(auc_str+'\\n')\n",
    "    f.write(recall_str+'\\n')\n",
    "    f.write(precision_str+'\\n')\n",
    "    f.write(f1_str+'\\n')\n",
    "    f.write(kappa_str+'\\n')\n",
    "    f.write(mcc_str+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf9e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-dgl]",
   "language": "python",
   "name": "conda-env-anaconda3-dgl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
