{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/dgllife/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from dgllife.utils import Meter, EarlyStopping\n",
    "from hyperopt import fmin, tpe\n",
    "from shutil import copyfile\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import hyperopt\n",
    "from hyper import init_hyper_space\n",
    "from utils import get_configure, mkdir_p, init_trial_path, \\\n",
    "    split_dataset, collate_molgraphs, load_model, predict, init_featurizer, load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_curve,roc_auc_score, confusion_matrix, precision_recall_curve, auc, mean_squared_error, \\\n",
    "    r2_score, mean_absolute_error,cohen_kappa_score,accuracy_score,f1_score,matthews_corrcoef,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# patienceNum = 50\n",
    "# batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllMeter(object):\n",
    "    def statistical(self):\n",
    "        y_test = pd.DataFrame(self.y_test)\n",
    "        y_predict = pd.DataFrame(self.y_predict)\n",
    "        fpr, tpr, threshold = roc_curve(y_test, y_predict)\n",
    "        auc_prc = auc(precision_recall_curve(y_test, y_predict, pos_label=1)[1],\n",
    "                              precision_recall_curve(y_test, y_predict, pos_label=1)[0])\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        output_tran = []\n",
    "        for x in y_predict[0]:\n",
    "            if x > 0.5:\n",
    "                output_tran.append(1)\n",
    "            else:\n",
    "                output_tran.append(0)\n",
    "        acc = accuracy_score(y_test, output_tran)\n",
    "        recall = recall_score(y_test, output_tran)\n",
    "        precision = precision_score(y_test, output_tran)\n",
    "        f1 = f1_score(y_test, output_tran)\n",
    "        kappa = cohen_kappa_score(y_test,output_tran)   \n",
    "        mcc = matthews_corrcoef(y_test,output_tran)\n",
    "        \n",
    "        c_mat = confusion_matrix(y_test, output_tran)            \n",
    "        tn, fp, fn, tp = list(c_mat.flatten())\n",
    "        se = tp / (tp + fn)\n",
    "        sp = tn / (tn + fp)\n",
    "        acc_ = (tp + tn) / (tn + fp + fn + tp)          \n",
    "        recall_ = se\n",
    "        precision_ = tp / (tp + fp)\n",
    "        f1_ = 2 * (precision * recall) / (precision + recall) # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        mcc_ = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        scores_dict['auc_prc'] = auc_prc\n",
    "        scores_dict['acc'] = acc\n",
    "        scores_dict['auc_roc'] = auc_roc\n",
    "        scores_dict['recall'] = recall\n",
    "        scores_dict['precision'] = precision\n",
    "        scores_dict['f1'] = f1\n",
    "        scores_dict['kappa'] = kappa\n",
    "        scores_dict['mcc'] = mcc \n",
    "\n",
    "        print(scores_dict) \n",
    "        print({'acc':acc_,'recall_':recall_,'precision_':precision_,'f1_':f1_,'mcc_':mcc_})\n",
    "        return scores_dict\n",
    "         \n",
    "    \n",
    "    def __init__(self, mean=None, std=None):\n",
    "        self.y_predict = []\n",
    "        self.y_test = []\n",
    "            \n",
    "    def update(self, output, label, mask=None):\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            for i in output:\n",
    "                self.y_predict.append(i)\n",
    "            for j in label:\n",
    "                self.y_test.append(j)        \n",
    "    def compute_metric(self, metric_name, reduction='mean'):\n",
    "        if metric_name == 'getAllMetrics':\n",
    "            return self.statistical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    all_train_meter = AllMeter()\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        smiles, bg, labels, masks = batch_data\n",
    "        if len(smiles) == 1:\n",
    "            # Avoid potential issues with batch normalization\n",
    "            continue\n",
    "\n",
    "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
    "        logits = predict(args, model, bg)\n",
    "        # Mask non-existing labels\n",
    "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_meter.update(logits, labels, masks)\n",
    "        all_train_meter.update(logits, labels)\n",
    "        if batch_id % args['print_every'] == 0:\n",
    "            print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
    "                epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
    "    train_score = np.mean(train_meter.compute_metric(args['metric']))\n",
    "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric'], train_score))\n",
    "    roc_score = np.mean(train_meter.compute_metric(args['metric']))  \n",
    "    prc_score = np.mean(train_meter.compute_metric('pr_auc_score'))  \n",
    "    all_score = all_train_meter.compute_metric('getAllMetrics') \n",
    "#     return {'roc_auc_score': roc_score, 'pr_auc_score': prc_score, 'all_score': all_score}\n",
    "\n",
    "def run_an_eval_epoch(args, model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    all_eval_meter = AllMeter()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            labels = labels.to(args['device'])\n",
    "            logits = predict(args, model, bg)\n",
    "            eval_meter.update(logits, labels, masks)\n",
    "            all_eval_meter.update(logits, labels, masks)\n",
    "    roc_score = np.mean(eval_meter.compute_metric(args['metric']))  # in case of multi-tasks\n",
    "    prc_score = np.mean(eval_meter.compute_metric('pr_auc_score'))  # in case of multi-task\n",
    "    all_score = all_eval_meter.compute_metric('getAllMetrics') \n",
    "    return {'roc_auc_score': roc_score, 'pr_auc_score': prc_score, 'all_score': all_score}\n",
    "#     return np.mean(eval_meter.compute_metric(args['metric']))\n",
    "\n",
    "def main(args, exp_config, train_set, val_set, test_set):\n",
    "    # Record settings\n",
    "    exp_config.update({\n",
    "        'model': args['model'],\n",
    "        'n_tasks': args['n_tasks'],\n",
    "        'atom_featurizer_type': args['atom_featurizer_type'],\n",
    "        'bond_featurizer_type': args['bond_featurizer_type'],\n",
    "#         'patience': patienceNum,\n",
    "#         'batch_size':batch_size\n",
    "    })\n",
    "    if args['atom_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "    if args['edge_featurizer'] is not None and args['bond_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "\n",
    "    # Set up directory for saving results\n",
    "    args = init_trial_path(args)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=exp_config['batch_size'], shuffle=True,\n",
    "                              collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=exp_config['batch_size'],\n",
    "                            collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=exp_config['batch_size'],\n",
    "                             collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    model = load_model(exp_config).to(args['device'])\n",
    "\n",
    "    loss_criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "    stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                            filename=args['trial_path'] + '/model.pth',\n",
    "                            metric=args['metric'])\n",
    "\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "        # Validation and early stop\n",
    "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        early_stop = stopper.step(val_score['roc_auc_score'], model)\n",
    "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
    "            epoch + 1, args['num_epochs'], args['metric'],\n",
    "            val_score['roc_auc_score'], args['metric'], stopper.best_score))\n",
    "\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    test_score = run_an_eval_epoch(args, model, test_loader)\n",
    "    print('test {} {:.4f}'.format(args['metric'], test_score['roc_auc_score']))\n",
    "\n",
    "    with open(args['trial_path'] + '/eval.txt', 'w') as f:\n",
    "        f.write('Best val {}: {}\\n'.format(args['metric'], stopper.best_score))\n",
    "        f.write('Test {}: {}\\n'.format(args['metric'], test_score['roc_auc_score']))\n",
    "\n",
    "    with open(args['trial_path'] + '/configure.json', 'w') as f:\n",
    "        json.dump(exp_config, f, indent=2)\n",
    "\n",
    "    return args['trial_path'], stopper.best_score\n",
    "\n",
    "def bayesian_optimization(args, train_set, val_set, test_set):\n",
    "    # Run grid search\n",
    "    results = []\n",
    "\n",
    "    candidate_hypers = init_hyper_space(args['model'])\n",
    "\n",
    "    def objective(hyperparams):\n",
    "        configure = deepcopy(args)\n",
    "        trial_path, val_metric = main(configure, hyperparams, train_set, val_set, test_set)\n",
    "\n",
    "        if args['metric'] in ['roc_auc_score', 'pr_auc_score']:\n",
    "            # Maximize ROCAUC is equivalent to minimize the negative of it\n",
    "            val_metric_to_minimize = -1 * val_metric\n",
    "        else:\n",
    "            val_metric_to_minimize = val_metric\n",
    "\n",
    "        results.append((trial_path, val_metric_to_minimize))\n",
    "\n",
    "        return val_metric_to_minimize\n",
    "\n",
    "    fmin(objective, candidate_hypers, algo=tpe.suggest, max_evals=args['num_evals'])\n",
    "    results.sort(key=lambda tup: tup[1])\n",
    "    best_trial_path, best_val_metric = results[0]\n",
    "\n",
    "    return best_trial_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['-au', '--augmentation'], dest='augmentation', nargs=0, const=True, default=False, type=None, choices=None, help='Whether to augmentation', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-c', '--csv-path', type=str, required=True,\n",
    "                    help='Path to a csv file for loading a dataset')\n",
    "parser.add_argument('-sc', '--smiles-column', type=str, required=True,\n",
    "                    help='Header for the SMILES column in the CSV file')\n",
    "parser.add_argument('-lv', '--log-values', action='store_true', default=False,\n",
    "                    help='Whether to take logarithm of the labels for modeling')\n",
    "parser.add_argument('-t', '--task-names', default=None, type=str,\n",
    "                    help='Header for the tasks to model. If None, we will model '\n",
    "                         'all the columns except for the smiles_column in the CSV file. '\n",
    "                         '(default: None)')\n",
    "parser.add_argument('-s', '--split',\n",
    "                    choices=['scaffold_decompose', 'scaffold_smiles', 'random'],\n",
    "                    default='scaffold_smiles',\n",
    "                    help='Dataset splitting method (default: scaffold_smiles). For scaffold '\n",
    "                         'split based on rdkit.Chem.AllChem.MurckoDecompose, '\n",
    "                         'use scaffold_decompose. For scaffold split based on '\n",
    "                         'rdkit.Chem.Scaffolds.MurckoScaffold.MurckoScaffoldSmiles, '\n",
    "                         'use scaffold_smiles.')\n",
    "parser.add_argument('-sr', '--split-ratio', default='0.8,0.1,0.1', type=str,\n",
    "                    help='Proportion of the dataset to use for training, validation and test '\n",
    "                         '(default: 0.8,0.1,0.1)')\n",
    "parser.add_argument('-me', '--metric', choices=['roc_auc_score', 'pr_auc_score'],\n",
    "                        default='roc_auc_score',\n",
    "                        help='Metric for evaluation (default: roc_auc_score)')\n",
    "parser.add_argument('-mo', '--model', choices=['GCN', 'GAT', 'Weave', 'MPNN', 'AttentiveFP',\n",
    "                                               'gin_supervised_contextpred',\n",
    "                                               'gin_supervised_infomax',\n",
    "                                               'gin_supervised_edgepred',\n",
    "                                               'gin_supervised_masking',\n",
    "                                               'NF'],\n",
    "                    default='GCN', help='Model to use (default: GCN)')\n",
    "parser.add_argument('-a', '--atom-featurizer-type', choices=['canonical', 'attentivefp'],\n",
    "                    default='canonical',\n",
    "                    help='Featurization for atoms (default: canonical)')\n",
    "parser.add_argument('-b', '--bond-featurizer-type', choices=['canonical', 'attentivefp'],\n",
    "                    default='canonical',\n",
    "                    help='Featurization for bonds (default: canonical)')\n",
    "parser.add_argument('-n', '--num-epochs', type=int, default=1000,\n",
    "                    help='Maximum number of epochs allowed for training. '\n",
    "                         'We set a large number by default as early stopping '\n",
    "                         'will be performed. (default: 1000)')\n",
    "parser.add_argument('-nw', '--num-workers', type=int, default=0,\n",
    "                    help='Number of processes for data loading (default: 0)')\n",
    "parser.add_argument('-pe', '--print-every', type=int, default=20,\n",
    "                    help='Print the training progress every X mini-batches')\n",
    "parser.add_argument('-p', '--result-path', type=str, default='regression_results',\n",
    "                    help='Path to save training results (default: regression_results)')\n",
    "parser.add_argument('-ne', '--num-evals', type=int, default=None,\n",
    "                    help='Number of trials for hyperparameter search (default: None)')\n",
    "parser.add_argument('-au', '--augmentation', action='store_true', default=False,\n",
    "                    help='Whether to augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'csv_path': 'data/0-CF-2274.csv',\n",
       " 'smiles_column': 'SMILES',\n",
       " 'log_values': False,\n",
       " 'task_names': 'active_label',\n",
       " 'split': 'random',\n",
       " 'split_ratio': '0.8,0.1,0.1',\n",
       " 'metric': 'roc_auc_score',\n",
       " 'model': 'AttentiveFP',\n",
       " 'atom_featurizer_type': 'canonical',\n",
       " 'bond_featurizer_type': 'canonical',\n",
       " 'num_epochs': 300,\n",
       " 'num_workers': 0,\n",
       " 'print_every': 20,\n",
       " 'result_path': 'result/CF-2274_NoAug_AFP_20220906',\n",
       " 'num_evals': 50,\n",
       " 'augmentation': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPUNum = '0'\n",
    "repetitions = 50\n",
    "seed = 0 \n",
    "args = parser.parse_args(args=['--csv-path','data/0-CF-2274.csv',\n",
    "                               '--task-names','active_label',\n",
    "                               '--smiles-column','SMILES',\n",
    "                               '--result-path','result/CF-2274_NoAug_AFP_20220906',\n",
    "                               '--num-evals','50',\n",
    "                               '--num-epochs','300',\n",
    "#                                '--split-ratio',\n",
    "                                '--split','random',                     \n",
    "                               '--metric','roc_auc_score',\n",
    "                               '--model','AttentiveFP',\n",
    "#                                '--atom-featurizer-type','attentivefp',\n",
    "#                                '--bond-featurizer-type','attentivefp',\n",
    "#                                  '--augmentation',\n",
    "#                                '--num-workers',\n",
    "#                                '--print-every',\n",
    "                                  ]).__dict__\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def del_file(filepath):\n",
    "    del_list = os.listdir(filepath)\n",
    "    for f in del_list:\n",
    "        file_path = os.path.join(filepath, f)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "            \n",
    "path_data = args['result_path']\n",
    "\n",
    "if not os.path.exists(path_data):\n",
    "    os.makedirs(path_data)\n",
    "\n",
    "del_file(path_data)\n",
    "\n",
    "dirs = args['result_path']+'/saved_model'\n",
    "\n",
    "if not os.path.exists(dirs):\n",
    "    os.makedirs(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from maxsmi.augmentation_strategies import no_augmentation\n",
    "from maxsmi.augmentation_strategies import augmentation_with_duplication\n",
    "from maxsmi.augmentation_strategies import augmentation_without_duplication\n",
    "from maxsmi.augmentation_strategies import (\n",
    "    augmentation_with_reduced_duplication\n",
    ")\n",
    "from maxsmi.augmentation_strategies import augmentation_maximum_estimation\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def data_augmentation(dataSet,args):\n",
    "    smi_col = args['smiles_column']\n",
    "    task_names = args['task_names'][0]\n",
    "    df_empty = pd.DataFrame(columns=[smi_col,task_names])\n",
    "    for index,row in dataSet.iterrows():\n",
    "        smiles = row[smi_col]\n",
    "        non_duplicated_smiles = augmentation_with_reduced_duplication(smiles, 50)\n",
    "        for data in non_duplicated_smiles:\n",
    "            df = pd.DataFrame({smi_col:data,task_names:float(row[task_names])},index=[0])\n",
    "            df_empty = df_empty.append(df,ignore_index=True)\n",
    "    return df_empty\n",
    "\n",
    "def split_dataset_augmentation(my_df,seed,args):\n",
    "    \n",
    "    training_data, data_test = train_test_split(my_df, test_size=0.1, random_state=seed)\n",
    "    data_train, data_val = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "    \n",
    "    data_test = data_augmentation(data_test,args)\n",
    "    data_test = shuffle(data_test,random_state=seed) \n",
    "    data_train = data_augmentation(data_train,args)\n",
    "    data_train = shuffle(data_train,random_state=seed)\n",
    "    data_val = data_augmentation(data_val,args)\n",
    "    data_val = shuffle(data_val,random_state=seed)\n",
    "    \n",
    "    test_set = load_dataset(args, data_test)\n",
    "    train_set = load_dataset(args, data_train)\n",
    "    val_set = load_dataset(args, data_val)\n",
    "       \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory result/CF-2274_NoAug_AFP_20220906 already exists.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda:'+ GPUNum)\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "if args['task_names'] is not None:\n",
    "    args['task_names'] = args['task_names'].split(',')\n",
    "\n",
    "args = init_featurizer(args)\n",
    "df = pd.read_csv(args['csv_path'])\n",
    "mkdir_p(args['result_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/2274\n",
      "Processing molecule 2000/2274\n"
     ]
    }
   ],
   "source": [
    "if args['augmentation']:\n",
    "    train_set, val_set, test_set = split_dataset_augmentation(df,seed,args)\n",
    "    args['n_tasks'] = train_set.n_tasks\n",
    "else:\n",
    "    dataset = load_dataset(args, df)\n",
    "    train_set, val_set, test_set = split_dataset(args, dataset,seed)\n",
    "    args['n_tasks'] = dataset.n_tasks\n",
    "    \n",
    "# Whether to take the logarithm of labels for narrowing the range of values\n",
    "if args['log_values']:\n",
    "    train_set.labels = train_set.labels.log()\n",
    "    val_set.labels = val_set.labels.log()\n",
    "    test_set.labels = test_set.labels.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['num_evals'] is not None:\n",
    "    assert args['num_evals'] > 0, 'Expect the number of hyperparameter search trials to ' \\\n",
    "                                  'be greater than 0, got {:d}'.format(args['num_evals'])\n",
    "    print('Start hyperparameter search with Bayesian '\n",
    "          'optimization for {:d} trials'.format(args['num_evals']))\n",
    "    trial_path = bayesian_optimization(args, train_set, val_set, test_set)\n",
    "else:\n",
    "    print('Use the manually specified hyperparameters')\n",
    "    exp_config = get_configure(args['model'])\n",
    "    main(args, exp_config, train_set, val_set, test_set)\n",
    "    trial_path = args['result_path'] + '/1'\n",
    "\n",
    "# Copy final\n",
    "copyfile(trial_path + '/model.pth', args['result_path'] + '/model.pth')\n",
    "copyfile(trial_path + '/configure.json', args['result_path'] + '/configure.json')\n",
    "copyfile(trial_path + '/eval.txt', args['result_path'] + '/eval.txt')\n",
    "\n",
    "with open(args['result_path']+'/configure.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyper file: result/CF-2274_NoAug_AFP_20220906/39\n"
     ]
    }
   ],
   "source": [
    "print('best hyper file: '+ trial_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 256,\n",
       " 'dropout': 0.07196639428400786,\n",
       " 'graph_feat_size': 128,\n",
       " 'lr': 0.0012969611241468691,\n",
       " 'num_layers': 2,\n",
       " 'num_timesteps': 2,\n",
       " 'patience': 640,\n",
       " 'weight_decay': 1.4137588923528654e-05,\n",
       " 'model': 'AttentiveFP',\n",
       " 'n_tasks': 1,\n",
       " 'atom_featurizer_type': 'canonical',\n",
       " 'bond_featurizer_type': 'canonical',\n",
       " 'in_node_feats': 74,\n",
       " 'in_edge_feats': 13}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWithHyper (args, exp_config, train_set, val_set, test_set):\n",
    "\n",
    "    # Record settings\n",
    "    exp_config.update({\n",
    "        'model': args['model'],\n",
    "        'n_tasks': args['n_tasks'],\n",
    "        'atom_featurizer_type': args['atom_featurizer_type'],\n",
    "        'bond_featurizer_type': args['bond_featurizer_type'],\n",
    "#         'patience': patienceNum,\n",
    "#         'batch_size':batch_size\n",
    "    })\n",
    "    if args['atom_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "    if args['edge_featurizer'] is not None and args['bond_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "\n",
    "    # Set up directory for saving results\n",
    "#     args = init_trial_path(args)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=exp_config['batch_size'], shuffle=True,\n",
    "                              collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=exp_config['batch_size'],\n",
    "                            collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=exp_config['batch_size'],\n",
    "                             collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    model = load_model(exp_config).to(args['device'])\n",
    "    \n",
    "    best_model_file = args['result_path']+'/saved_model/%s_bst_%s.pth' % (args['model'], split)\n",
    "\n",
    "#     loss_criterion = nn.SmoothL1Loss(reduction='none')\n",
    "#     optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "#                      weight_decay=exp_config['weight_decay'])\n",
    "    \n",
    "#     stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "#                             filename=best_model_file,\n",
    "#                             metric=args['metric'])\n",
    "    loss_criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "    stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                            filename=best_model_file,\n",
    "                            metric=args['metric'])\n",
    "    for epoch in range(args['num_epochs']):\n",
    "#         # Train\n",
    "#         run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "#         # Validation and early stop\n",
    "#         val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "#         early_stop = stopper.step(val_score[args['metric']], model)\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "        # Validation and early stop\n",
    "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        early_stop = stopper.step(val_score['roc_auc_score'], model)\n",
    "\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    \n",
    "    tr_scores = run_an_eval_epoch(args, model, train_loader)\n",
    "    val_scores = run_an_eval_epoch(args, model, val_loader)\n",
    "    te_scores = run_an_eval_epoch(args, model, test_loader)\n",
    "    \n",
    "    return tr_scores,val_scores,te_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_res = []\n",
    "val_res = []\n",
    "te_res = []\n",
    "# for split in range(1, repetitions + 1):\n",
    "for split in range(1, repetitions + 1):\n",
    "    \n",
    "    if args['augmentation']:\n",
    "        train_set, val_set, test_set = split_dataset_augmentation(df, split,args)\n",
    "        args['n_tasks'] = train_set.n_tasks\n",
    "    else:\n",
    "        train_set, val_set, test_set = split_dataset(args,dataset,split)\n",
    "        args['n_tasks'] = dataset.n_tasks\n",
    "\n",
    "    # Whether to take the logarithm of labels for narrowing the range of values\n",
    "    if args['log_values']:\n",
    "        train_set.labels = train_set.labels.log()\n",
    "        val_set.labels = val_set.labels.log()\n",
    "        test_set.labels = test_set.labels.log()\n",
    "    print('n_tasks : '+ str(args['n_tasks']))\n",
    "\n",
    "    tr_scores,val_scores,te_scores = trainWithHyper(args, config, train_set, val_set, test_set)\n",
    "\n",
    "    tr_res.append(tr_scores);\n",
    "    val_res.append(val_scores);\n",
    "    te_res.append(te_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList(res):\n",
    "    auc_prc_list = []\n",
    "    acc_list = []\n",
    "    auc_roc_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    f1_list = []\n",
    "    kappa_list = []\n",
    "    mcc_list = []\n",
    "    for item in res:\n",
    "        auc_prc_list.append(item['all_score']['auc_prc'])\n",
    "        acc_list.append(item['all_score']['acc'])\n",
    "        auc_roc_list.append(item['all_score']['auc_roc'])\n",
    "        recall_list.append(item['all_score']['recall'])\n",
    "        precision_list.append(item['all_score']['precision'])\n",
    "        f1_list.append(item['all_score']['f1'])\n",
    "        kappa_list.append(item['all_score']['kappa'])\n",
    "        mcc_list.append(item['all_score']['mcc'])\n",
    "    return auc_prc_list,acc_list,auc_roc_list,recall_list,precision_list,f1_list,kappa_list,mcc_list\n",
    "\n",
    "tr_auc_prc_list,tr_acc_list,tr_auc_roc_list,tr_recall_list,tr_precision_list,tr_f1_list,tr_kappa_list,tr_mcc_list = getList(tr_res)\n",
    "val_auc_prc_list,val_acc_list,val_auc_roc_list,val_recall_list,val_precision_list,val_f1_list,val_kappa_list,val_mcc_list = getList(val_res)\n",
    "te_auc_prc_list,te_acc_list,te_auc_roc_list,te_recall_list,te_precision_list,te_f1_list,te_kappa_list,te_mcc_list = getList(te_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # acc auc_roc recall precision f1 kappa mcc\n",
    "    acc_str = 'acc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_acc_list), \n",
    "                    np.std(tr_acc_list),\n",
    "                    np.mean(val_acc_list), \n",
    "                    np.std(val_acc_list),\n",
    "                    np.mean(te_acc_list), \n",
    "                    np.std(te_acc_list),\n",
    "    )\n",
    "    auc_str = 'auc_roc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_auc_roc_list), \n",
    "                    np.std(tr_auc_roc_list),\n",
    "                    np.mean(val_auc_roc_list), \n",
    "                    np.std(val_auc_roc_list),\n",
    "                    np.mean(te_auc_roc_list), \n",
    "                    np.std(te_auc_roc_list),\n",
    "    )\n",
    "    recall_str = 'recall of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_recall_list), \n",
    "                    np.std(tr_recall_list),\n",
    "                    np.mean(val_recall_list), \n",
    "                    np.std(val_recall_list),\n",
    "                    np.mean(te_recall_list), \n",
    "                    np.std(te_recall_list),\n",
    "    )\n",
    "    precision_str = 'precision of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_precision_list), \n",
    "                    np.std(tr_precision_list),\n",
    "                    np.mean(val_precision_list), \n",
    "                    np.std(val_precision_list),\n",
    "                    np.mean(te_precision_list), \n",
    "                    np.std(te_precision_list),\n",
    "    )\n",
    "    f1_str = 'f1 of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_f1_list), \n",
    "                    np.std(tr_f1_list),\n",
    "                    np.mean(val_f1_list), \n",
    "                    np.std(val_f1_list),\n",
    "                    np.mean(te_f1_list), \n",
    "                    np.std(te_f1_list),\n",
    "    )\n",
    "    kappa_str = 'kappa of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_kappa_list), \n",
    "                    np.std(tr_kappa_list),\n",
    "                    np.mean(val_kappa_list), \n",
    "                    np.std(val_kappa_list),\n",
    "                    np.mean(te_kappa_list), \n",
    "                    np.std(te_kappa_list),\n",
    "    )\n",
    "    mcc_str = 'mcc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_mcc_list), \n",
    "                    np.std(tr_mcc_list),\n",
    "                    np.mean(val_mcc_list), \n",
    "                    np.std(val_mcc_list),\n",
    "                    np.mean(te_mcc_list), \n",
    "                    np.std(te_mcc_list),\n",
    "    )\n",
    "    auc_prc_str = 'auc_prc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_auc_prc_list), \n",
    "                    np.std(tr_auc_prc_list),\n",
    "                    np.mean(val_auc_prc_list), \n",
    "                    np.std(val_auc_prc_list),\n",
    "                    np.mean(te_auc_prc_list), \n",
    "                    np.std(te_auc_prc_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of training set is 0.914±0.045, validation set is 0.868±0.021, test set is 0.853±0.027\n",
      "auc_roc of training set is 0.930±0.049, validation set is 0.874±0.028, test set is 0.837±0.040\n",
      "recall of training set is 0.628±0.232, validation set is 0.499±0.151, test set is 0.466±0.146\n",
      "precision of training set is 0.863±0.095, validation set is 0.716±0.106, test set is 0.678±0.119\n",
      "f1 of training set is 0.704±0.178, validation set is 0.565±0.100, test set is 0.528±0.110\n",
      "kappa of training set is 0.659±0.198, validation set is 0.493±0.098, test set is 0.449±0.109\n",
      "mcc of training set is 0.682±0.177, validation set is 0.516±0.080, test set is 0.471±0.094\n",
      "auc_prc of training set is 0.827±0.116, validation set is 0.667±0.068, test set is 0.638±0.076\n",
      "AttentiveFP\n"
     ]
    }
   ],
   "source": [
    "print(acc_str)\n",
    "print(auc_str)\n",
    "print(recall_str)\n",
    "print(precision_str)\n",
    "print(f1_str)\n",
    "print(kappa_str)\n",
    "print(mcc_str)\n",
    "print(auc_prc_str)\n",
    "print(args['model'])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time 2022.09.06 03:18:16\n",
      "end_time 2022.09.06 04:41:54\n"
     ]
    }
   ],
   "source": [
    "start_time_arr = time.localtime(start_time)\n",
    "start_time_Style = time.strftime('%Y.%m.%d %H:%M:%S',start_time_arr)\n",
    "print('start_time', start_time_Style)\n",
    "end_time_arr = time.localtime(end_time)\n",
    "end_time_Style = time.strftime('%Y.%m.%d %H:%M:%S',end_time_arr)\n",
    "print('end_time', end_time_Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args['result_path'] + '/output.txt', 'w') as f:\n",
    "    f.write(acc_str+'\\n')\n",
    "    f.write(auc_str+'\\n')\n",
    "    f.write(recall_str+'\\n')\n",
    "    f.write(precision_str+'\\n')\n",
    "    f.write(f1_str+'\\n')\n",
    "    f.write(kappa_str+'\\n')\n",
    "    f.write(mcc_str+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model: AttentiveFP</th>\n",
       "      <th>Train</th>\n",
       "      <th>Tr_STD</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Va_STD</th>\n",
       "      <th>Test</th>\n",
       "      <th>Te_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc</td>\n",
       "      <td>0.914206</td>\n",
       "      <td>0.045334</td>\n",
       "      <td>0.868458</td>\n",
       "      <td>0.020606</td>\n",
       "      <td>0.852895</td>\n",
       "      <td>0.026797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc_roc</td>\n",
       "      <td>0.930254</td>\n",
       "      <td>0.049297</td>\n",
       "      <td>0.873754</td>\n",
       "      <td>0.028156</td>\n",
       "      <td>0.837062</td>\n",
       "      <td>0.040331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.627598</td>\n",
       "      <td>0.232457</td>\n",
       "      <td>0.499062</td>\n",
       "      <td>0.150909</td>\n",
       "      <td>0.465883</td>\n",
       "      <td>0.146301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.863387</td>\n",
       "      <td>0.095134</td>\n",
       "      <td>0.716406</td>\n",
       "      <td>0.106416</td>\n",
       "      <td>0.678256</td>\n",
       "      <td>0.119124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.704418</td>\n",
       "      <td>0.178402</td>\n",
       "      <td>0.564570</td>\n",
       "      <td>0.100274</td>\n",
       "      <td>0.528391</td>\n",
       "      <td>0.109657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kappa</td>\n",
       "      <td>0.659238</td>\n",
       "      <td>0.197855</td>\n",
       "      <td>0.493327</td>\n",
       "      <td>0.098120</td>\n",
       "      <td>0.448887</td>\n",
       "      <td>0.109177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mcc</td>\n",
       "      <td>0.681930</td>\n",
       "      <td>0.177232</td>\n",
       "      <td>0.516278</td>\n",
       "      <td>0.079949</td>\n",
       "      <td>0.470980</td>\n",
       "      <td>0.093784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>auc_prc</td>\n",
       "      <td>0.827376</td>\n",
       "      <td>0.116214</td>\n",
       "      <td>0.667098</td>\n",
       "      <td>0.067722</td>\n",
       "      <td>0.637938</td>\n",
       "      <td>0.075762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model: AttentiveFP     Train    Tr_STD  Validation    Va_STD      Test  \\\n",
       "0                acc  0.914206  0.045334    0.868458  0.020606  0.852895   \n",
       "1            auc_roc  0.930254  0.049297    0.873754  0.028156  0.837062   \n",
       "2             recall  0.627598  0.232457    0.499062  0.150909  0.465883   \n",
       "3          precision  0.863387  0.095134    0.716406  0.106416  0.678256   \n",
       "4                 f1  0.704418  0.178402    0.564570  0.100274  0.528391   \n",
       "5              kappa  0.659238  0.197855    0.493327  0.098120  0.448887   \n",
       "6                mcc  0.681930  0.177232    0.516278  0.079949  0.470980   \n",
       "7            auc_prc  0.827376  0.116214    0.667098  0.067722  0.637938   \n",
       "\n",
       "     Te_STD  \n",
       "0  0.026797  \n",
       "1  0.040331  \n",
       "2  0.146301  \n",
       "3  0.119124  \n",
       "4  0.109657  \n",
       "5  0.109177  \n",
       "6  0.093784  \n",
       "7  0.075762  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_acc_list,tr_auc_roc_list,tr_recall_list,tr_precision_list,tr_f1_list,tr_kappa_list,tr_mcc_list,tr_auc_prc_list\n",
    "val_acc_list,val_auc_roc_list,val_recall_list,val_precision_list,val_f1_list,val_kappa_list,val_mcc_list,tr_auc_prc_list\n",
    "te_acc_list,te_auc_roc_list,te_recall_list,te_precision_list,te_f1_list,te_kappa_list,te_mcc_list,tr_auc_prc_list\n",
    "\n",
    "import pandas as pd\n",
    "import collections\n",
    "dict1 = {\"model: \"+args['model']:['acc','auc_roc','recall','precision','f1','kappa','mcc','auc_prc'],\n",
    "         \"Train\":[np.mean(tr_acc_list),np.mean(tr_auc_roc_list),np.mean(tr_recall_list),np.mean(tr_precision_list), \n",
    "                  np.mean(tr_f1_list),np.mean(tr_kappa_list), np.mean(tr_mcc_list),np.mean(tr_auc_prc_list)],\n",
    "         \"Tr_STD\":[np.std(tr_acc_list),np.std(tr_auc_roc_list),np.std(tr_recall_list),np.std(tr_precision_list), \n",
    "                  np.std(tr_f1_list),np.std(tr_kappa_list), np.std(tr_mcc_list),np.std(tr_auc_prc_list)],\n",
    "         \"Validation\":[np.mean(val_acc_list),np.mean(val_auc_roc_list),np.mean(val_recall_list),np.mean(val_precision_list), \n",
    "                  np.mean(val_f1_list),np.mean(val_kappa_list), np.mean(val_mcc_list),np.mean(val_auc_prc_list)],\n",
    "         \"Va_STD\":[np.std(val_acc_list),np.std(val_auc_roc_list),np.std(val_recall_list),np.std(val_precision_list), \n",
    "                  np.std(val_f1_list),np.std(val_kappa_list), np.std(val_mcc_list),np.std(val_auc_prc_list)],\n",
    "         \"Test\":[np.mean(te_acc_list),np.mean(te_auc_roc_list),np.mean(te_recall_list),np.mean(te_precision_list), \n",
    "                  np.mean(te_f1_list),np.mean(te_kappa_list), np.mean(te_mcc_list),np.mean(te_auc_prc_list)],\n",
    "          \"Te_STD\":[np.std(te_acc_list),np.std(te_auc_roc_list),np.std(te_recall_list),np.std(te_precision_list), \n",
    "                  np.std(te_f1_list),np.std(te_kappa_list), np.std(te_mcc_list),np.std(te_auc_prc_list)]}\n",
    "dict1 = collections.OrderedDict(dict1)\n",
    "df = pd.DataFrame(dict1,index = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(args['result_path'] + '/output.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result/CF-2274_NoAug_AFP_20220906'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['result_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyper file: result/CF-2274_NoAug_AFP_20220906/39\n"
     ]
    }
   ],
   "source": [
    "print('best hyper file: '+ trial_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from dgllife.data import UnlabeledSMILES\n",
    "from dgllife.utils import mol_to_bigraph\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import mkdir_p, collate_molgraphs_unlabeled, load_model, predict, init_featurizer\n",
    "\n",
    "def main(args):\n",
    "    dataset = UnlabeledSMILES(args['smiles'], node_featurizer=args['node_featurizer'],\n",
    "                              edge_featurizer=args['edge_featurizer'],\n",
    "                              mol_to_graph=partial(mol_to_bigraph, add_self_loop=True))\n",
    "    dataloader = DataLoader(dataset, batch_size=args['batch_size'],\n",
    "                            collate_fn=collate_molgraphs_unlabeled, num_workers=args['num_workers'])\n",
    "    model = load_model(args).to(args['device'])\n",
    "    checkpoint = torch.load(args['train_result_path'] + '/model.pth', map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    smiles_list = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(tqdm(dataloader, desc=\"Iteration\")):\n",
    "            batch_smiles, bg = batch_data\n",
    "            smiles_list.extend(batch_smiles)\n",
    "            batch_pred = predict(args, model, bg)\n",
    "            if not args['soft_classification']:\n",
    "                batch_pred = (batch_pred >= 0.5).float()\n",
    "            predictions.append(batch_pred.detach().cpu())\n",
    "\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "    output_data = {'canonical_smiles': smiles_list}\n",
    "    if args['task_names'] is None:\n",
    "        args['task_names'] = ['task_{:d}'.format(t) for t in range(1, args['n_tasks'] + 1)]\n",
    "    else:\n",
    "        args['task_names'] = args['task_names'].split(',')\n",
    "    for task_id, task_name in enumerate(args['task_names']):\n",
    "        output_data[task_name] = predictions[:, task_id]\n",
    "    df = pd.DataFrame(output_data)\n",
    "    df.to_csv(args['inference_result_path'] + '/prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-f', '--file-path', type=str, required=True,\n",
    "                    help='Path to a .csv/.txt file of SMILES strings')\n",
    "parser.add_argument('-sc', '--smiles-column', type=str,\n",
    "                    help='Header for the SMILES column in the CSV file, can be '\n",
    "                         'omitted if the input file is a .txt file or the .csv '\n",
    "                         'file only has one column of SMILES strings')\n",
    "parser.add_argument('-tp', '--train-result-path', type=str, default='classification_results',\n",
    "                    help='Path to the saved training results, which will be used for '\n",
    "                         'loading the trained model and related configurations')\n",
    "parser.add_argument('-ip', '--inference-result-path', type=str, default='classification_inference_results',\n",
    "                    help='Path to save the inference results')\n",
    "parser.add_argument('-t', '--task-names', default=None, type=str,\n",
    "                    help='Task names for saving model predictions in the CSV file to output, '\n",
    "                         'which should be the same as the ones used for training. If not '\n",
    "                         'specified, we will simply use task1, task2, ...')\n",
    "parser.add_argument('-s', '--soft-classification', action='store_true', default=False,\n",
    "                    help='By default we will perform hard classification with binary labels. '\n",
    "                         'This flag allows performing soft classification instead.')\n",
    "parser.add_argument('-nw', '--num-workers', type=int, default=1,\n",
    "                    help='Number of processes for data loading (default: 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_interface = parser.parse_args(args=['--file-path','predict_data/14_AR.csv',\n",
    "                               '--smiles-column',args['smiles_column'],\n",
    "                               '--task-names',args['task_names'][0],\n",
    "                               '--train-result-path',args['result_path'],\n",
    "                               '--inference-result-path',args['result_path'],\n",
    "                               '--num-workers',args['num_workers'],                                \n",
    "                                  ]).__dict__\n",
    "args_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(args_interface['train_result_path'] + '/configure.json', 'r') as f:\n",
    "    args_interface.update(json.load(f))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args_interface['device'] = torch.device('cuda:0')\n",
    "else:\n",
    "    args_interface['device'] = torch.device('cpu')\n",
    "\n",
    "if args_interface['file_path'].endswith('.csv') or args_interface['file_path'].endswith('.csv.gz'):\n",
    "    import pandas\n",
    "    df = pandas.read_csv(args_interface['file_path'])\n",
    "    if args_interface['smiles_column'] is not None:\n",
    "        smiles = df[args_interface['smiles_column']].tolist()\n",
    "    else:\n",
    "        assert len(df.columns) == 1, 'The CSV file has more than 1 columns and ' \\\n",
    "                                     '-sc (smiles-column) needs to be specified.'\n",
    "        smiles = df[df.columns[0]].tolist()\n",
    "elif args_interface['file_path'].endswith('.txt'):\n",
    "    from dgllife.utils import load_smiles_from_txt\n",
    "    smiles = load_smiles_from_txt(args_interface['file_path'])\n",
    "else:\n",
    "    raise ValueError('Expect the input data file to be a .csv or a .txt file, '\n",
    "                     'got {}'.format(args_interface['file_path']))\n",
    "args_interface['smiles'] = smiles\n",
    "args_interface = init_featurizer(args_interface)\n",
    "\n",
    "# Handle directories\n",
    "mkdir_p(args_interface['inference_result_path'])\n",
    "assert os.path.exists(args_interface['train_result_path']), \\\n",
    "    'The path to the saved training results does not exist.'\n",
    "\n",
    "main(args_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dgllife]",
   "language": "python",
   "name": "conda-env-dgllife-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
