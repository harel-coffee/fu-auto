{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from dgllife.utils import Meter, EarlyStopping\n",
    "from hyperopt import fmin, tpe\n",
    "from shutil import copyfile\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyper import init_hyper_space\n",
    "from utils import get_configure, mkdir_p, init_trial_path, \\\n",
    "    split_dataset, collate_molgraphs, load_model, predict, init_featurizer, load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_curve,roc_auc_score, confusion_matrix, precision_recall_curve, auc, mean_squared_error, \\\n",
    "    r2_score, mean_absolute_error,cohen_kappa_score,accuracy_score,f1_score,matthews_corrcoef,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# patienceNum = 50\n",
    "# batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllMeter(object):\n",
    "    \n",
    "    def statistical(self):\n",
    "        \n",
    "        y_test = pd.DataFrame(self.y_test)\n",
    "        y_predict = pd.DataFrame(self.y_predict)\n",
    "        fpr, tpr, threshold = roc_curve(y_test, y_predict)\n",
    "        auc_prc = auc(precision_recall_curve(y_test, y_predict, pos_label=1)[1],\n",
    "                              precision_recall_curve(y_test, y_predict, pos_label=1)[0])\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        output_tran = []\n",
    "        for x in y_predict[0]:\n",
    "            if x > 0.5:\n",
    "                output_tran.append(1)\n",
    "            else:\n",
    "                output_tran.append(0)\n",
    "        acc = accuracy_score(y_test, output_tran)\n",
    "        recall = recall_score(y_test, output_tran)\n",
    "        precision = precision_score(y_test, output_tran)\n",
    "        f1 = f1_score(y_test, output_tran)\n",
    "        kappa = cohen_kappa_score(y_test,output_tran)   \n",
    "        mcc = matthews_corrcoef(y_test,output_tran)\n",
    "        \n",
    "        c_mat = confusion_matrix(y_test, output_tran)            \n",
    "        tn, fp, fn, tp = list(c_mat.flatten())\n",
    "        se = tp / (tp + fn)\n",
    "        sp = tn / (tn + fp)\n",
    "        acc_ = (tp + tn) / (tn + fp + fn + tp)          \n",
    "        recall_ = se\n",
    "        precision_ = tp / (tp + fp)\n",
    "        f1_ = 2 * (precision * recall) / (precision + recall) # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        mcc_ = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n",
    "    \n",
    "        scores_dict = {}\n",
    "        scores_dict['auc_prc'] = auc_prc\n",
    "        scores_dict['acc'] = acc\n",
    "        scores_dict['auc_roc'] = auc_roc\n",
    "        scores_dict['recall'] = recall\n",
    "        scores_dict['precision'] = precision\n",
    "        scores_dict['f1'] = f1\n",
    "        scores_dict['kappa'] = kappa\n",
    "        scores_dict['mcc'] = mcc \n",
    "\n",
    "        print(scores_dict) \n",
    "        print({'acc':acc_,'recall_':recall_,'precision_':precision_,'f1_':f1_,'mcc_':mcc_})\n",
    "        return scores_dict\n",
    "         \n",
    "    \n",
    "    def __init__(self, mean=None, std=None):\n",
    "        self.y_predict = []\n",
    "        self.y_test = []\n",
    "            \n",
    "    def update(self, output, label, mask=None):\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            for i in output:\n",
    "                self.y_predict.append(i)\n",
    "            for j in label:\n",
    "                self.y_test.append(j)        \n",
    "    def compute_metric(self, metric_name, reduction='mean'):\n",
    "        if metric_name == 'getAllMetrics':\n",
    "            return self.statistical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    all_train_meter = AllMeter()\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        smiles, bg, labels, masks = batch_data\n",
    "        if len(smiles) == 1:\n",
    "            # Avoid potential issues with batch normalization\n",
    "            continue\n",
    "\n",
    "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
    "        logits = predict(args, model, bg)\n",
    "        # Mask non-existing labels\n",
    "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_meter.update(logits, labels, masks)\n",
    "        all_train_meter.update(logits, labels)\n",
    "        if batch_id % args['print_every'] == 0:\n",
    "            print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
    "                epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
    "    train_score = np.mean(train_meter.compute_metric(args['metric']))\n",
    "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric'], train_score))\n",
    "    roc_score = np.mean(train_meter.compute_metric(args['metric']))  \n",
    "    prc_score = np.mean(train_meter.compute_metric('pr_auc_score'))  \n",
    "    all_score = all_train_meter.compute_metric('getAllMetrics') \n",
    "#     return {'roc_auc_score': roc_score, 'pr_auc_score': prc_score, 'all_score': all_score}\n",
    "\n",
    "def run_an_eval_epoch(args, model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    all_eval_meter = AllMeter()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            labels = labels.to(args['device'])\n",
    "            logits = predict(args, model, bg)\n",
    "            eval_meter.update(logits, labels, masks)\n",
    "            all_eval_meter.update(logits, labels, masks)\n",
    "    roc_score = np.mean(eval_meter.compute_metric(args['metric']))  # in case of multi-tasks\n",
    "    prc_score = np.mean(eval_meter.compute_metric('pr_auc_score'))  # in case of multi-task\n",
    "    all_score = all_eval_meter.compute_metric('getAllMetrics') \n",
    "    return {'roc_auc_score': roc_score, 'pr_auc_score': prc_score, 'all_score': all_score}\n",
    "#     return np.mean(eval_meter.compute_metric(args['metric']))\n",
    "\n",
    "def main(args, exp_config, train_set, val_set, test_set):\n",
    "    # Record settings\n",
    "    exp_config.update({\n",
    "        'model': args['model'],\n",
    "        'n_tasks': args['n_tasks'],\n",
    "        'atom_featurizer_type': args['atom_featurizer_type'],\n",
    "        'bond_featurizer_type': args['bond_featurizer_type'],\n",
    "#         'patience': patienceNum,\n",
    "#         'batch_size':batch_size\n",
    "    })\n",
    "    if args['atom_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "    if args['edge_featurizer'] is not None and args['bond_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "\n",
    "    # Set up directory for saving results\n",
    "    args = init_trial_path(args)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=exp_config['batch_size'], shuffle=True,\n",
    "                              collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=exp_config['batch_size'],\n",
    "                            collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=exp_config['batch_size'],\n",
    "                             collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    model = load_model(exp_config).to(args['device'])\n",
    "\n",
    "    loss_criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "    stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                            filename=args['trial_path'] + '/model.pth',\n",
    "                            metric=args['metric'])\n",
    "\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "        # Validation and early stop\n",
    "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        early_stop = stopper.step(val_score['roc_auc_score'], model)\n",
    "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
    "            epoch + 1, args['num_epochs'], args['metric'],\n",
    "            val_score['roc_auc_score'], args['metric'], stopper.best_score))\n",
    "\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    test_score = run_an_eval_epoch(args, model, test_loader)\n",
    "    print('test {} {:.4f}'.format(args['metric'], test_score['roc_auc_score']))\n",
    "\n",
    "    with open(args['trial_path'] + '/eval.txt', 'w') as f:\n",
    "        f.write('Best val {}: {}\\n'.format(args['metric'], stopper.best_score))\n",
    "        f.write('Test {}: {}\\n'.format(args['metric'], test_score['roc_auc_score']))\n",
    "\n",
    "    with open(args['trial_path'] + '/configure.json', 'w') as f:\n",
    "        json.dump(exp_config, f, indent=2)\n",
    "\n",
    "    return args['trial_path'], stopper.best_score\n",
    "\n",
    "def bayesian_optimization(args, train_set, val_set, test_set):\n",
    "    # Run grid search\n",
    "    results = []\n",
    "\n",
    "    candidate_hypers = init_hyper_space(args['model'])\n",
    "\n",
    "    def objective(hyperparams):\n",
    "        configure = deepcopy(args)\n",
    "        trial_path, val_metric = main(configure, hyperparams, train_set, val_set, test_set)\n",
    "\n",
    "        if args['metric'] in ['roc_auc_score', 'pr_auc_score']:\n",
    "            # Maximize ROCAUC is equivalent to minimize the negative of it\n",
    "            val_metric_to_minimize = -1 * val_metric\n",
    "        else:\n",
    "            val_metric_to_minimize = val_metric\n",
    "\n",
    "        results.append((trial_path, val_metric_to_minimize))\n",
    "\n",
    "        return val_metric_to_minimize\n",
    "\n",
    "    fmin(objective, candidate_hypers, algo=tpe.suggest, max_evals=args['num_evals'])\n",
    "    results.sort(key=lambda tup: tup[1])\n",
    "    best_trial_path, best_val_metric = results[0]\n",
    "\n",
    "    return best_trial_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['-au', '--augmentation'], dest='augmentation', nargs=0, const=True, default=False, type=None, choices=None, help='Whether to augmentation', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-c', '--csv-path', type=str, required=True,\n",
    "                    help='Path to a csv file for loading a dataset')\n",
    "parser.add_argument('-sc', '--smiles-column', type=str, required=True,\n",
    "                    help='Header for the SMILES column in the CSV file')\n",
    "parser.add_argument('-lv', '--log-values', action='store_true', default=False,\n",
    "                    help='Whether to take logarithm of the labels for modeling')\n",
    "parser.add_argument('-t', '--task-names', default=None, type=str,\n",
    "                    help='Header for the tasks to model. If None, we will model '\n",
    "                         'all the columns except for the smiles_column in the CSV file. '\n",
    "                         '(default: None)')\n",
    "parser.add_argument('-s', '--split',\n",
    "                    choices=['scaffold_decompose', 'scaffold_smiles', 'random'],\n",
    "                    default='scaffold_smiles',\n",
    "                    help='Dataset splitting method (default: scaffold_smiles). For scaffold '\n",
    "                         'split based on rdkit.Chem.AllChem.MurckoDecompose, '\n",
    "                         'use scaffold_decompose. For scaffold split based on '\n",
    "                         'rdkit.Chem.Scaffolds.MurckoScaffold.MurckoScaffoldSmiles, '\n",
    "                         'use scaffold_smiles.')\n",
    "parser.add_argument('-sr', '--split-ratio', default='0.8,0.1,0.1', type=str,\n",
    "                    help='Proportion of the dataset to use for training, validation and test '\n",
    "                         '(default: 0.8,0.1,0.1)')\n",
    "parser.add_argument('-me', '--metric', choices=['roc_auc_score', 'pr_auc_score'],\n",
    "                        default='roc_auc_score',\n",
    "                        help='Metric for evaluation (default: roc_auc_score)')\n",
    "parser.add_argument('-mo', '--model', choices=['GCN', 'GAT', 'Weave', 'MPNN', 'AttentiveFP',\n",
    "                                               'gin_supervised_contextpred',\n",
    "                                               'gin_supervised_infomax',\n",
    "                                               'gin_supervised_edgepred',\n",
    "                                               'gin_supervised_masking',\n",
    "                                               'NF'],\n",
    "                    default='GCN', help='Model to use (default: GCN)')\n",
    "parser.add_argument('-a', '--atom-featurizer-type', choices=['canonical', 'attentivefp'],\n",
    "                    default='canonical',\n",
    "                    help='Featurization for atoms (default: canonical)')\n",
    "parser.add_argument('-b', '--bond-featurizer-type', choices=['canonical', 'attentivefp'],\n",
    "                    default='canonical',\n",
    "                    help='Featurization for bonds (default: canonical)')\n",
    "parser.add_argument('-n', '--num-epochs', type=int, default=1000,\n",
    "                    help='Maximum number of epochs allowed for training. '\n",
    "                         'We set a large number by default as early stopping '\n",
    "                         'will be performed. (default: 1000)')\n",
    "parser.add_argument('-nw', '--num-workers', type=int, default=0,\n",
    "                    help='Number of processes for data loading (default: 0)')\n",
    "parser.add_argument('-pe', '--print-every', type=int, default=20,\n",
    "                    help='Print the training progress every X mini-batches')\n",
    "parser.add_argument('-p', '--result-path', type=str, default='regression_results',\n",
    "                    help='Path to save training results (default: regression_results)')\n",
    "parser.add_argument('-ne', '--num-evals', type=int, default=None,\n",
    "                    help='Number of trials for hyperparameter search (default: None)')\n",
    "parser.add_argument('-au', '--augmentation', action='store_true', default=False,\n",
    "                    help='Whether to augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom_featurizer_type': 'canonical',\n",
       " 'augmentation': False,\n",
       " 'bond_featurizer_type': 'canonical',\n",
       " 'csv_path': 'data/0-CF-2274.csv',\n",
       " 'log_values': False,\n",
       " 'metric': 'roc_auc_score',\n",
       " 'model': 'GAT',\n",
       " 'num_epochs': 300,\n",
       " 'num_evals': 50,\n",
       " 'num_workers': 0,\n",
       " 'print_every': 20,\n",
       " 'result_path': 'result/CF-2274_NoAug_GAT_20220906',\n",
       " 'smiles_column': 'SMILES',\n",
       " 'split': 'random',\n",
       " 'split_ratio': '0.8,0.1,0.1',\n",
       " 'task_names': 'active_label'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPUNum = '0'\n",
    "repetitions = 50\n",
    "seed = 0 \n",
    "args = parser.parse_args(args=['--csv-path','data/0-CF-2274.csv',\n",
    "                               '--task-names','active_label',\n",
    "                               '--smiles-column','SMILES',\n",
    "                               '--result-path','result/CF-2274_NoAug_GAT_20220906',\n",
    "                               '--num-evals','50',\n",
    "                               '--num-epochs','300',\n",
    "#                                '--split-ratio',\n",
    "                                '--split','random',                     \n",
    "                               '--metric','roc_auc_score',\n",
    "                               '--model','GAT',\n",
    "#                                '--atom-featurizer-type','attentivefp',\n",
    "#                                '--bond-featurizer-type','attentivefp',\n",
    "#                                  '--augmentation',\n",
    "#                                '--num-workers',\n",
    "#                                '--print-every',\n",
    "                                  ]).__dict__\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def del_file(filepath):\n",
    "    \"\"\"\n",
    "    :param filepath: 路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    del_list = os.listdir(filepath)\n",
    "    for f in del_list:\n",
    "        file_path = os.path.join(filepath, f)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "            \n",
    "path_data = args['result_path']\n",
    "\n",
    "if not os.path.exists(path_data):\n",
    "    os.makedirs(path_data)\n",
    "\n",
    "del_file(path_data)\n",
    "\n",
    "dirs = args['result_path']+'/saved_model'\n",
    "\n",
    "if not os.path.exists(dirs):\n",
    "    os.makedirs(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from maxsmi.augmentation_strategies import no_augmentation\n",
    "from maxsmi.augmentation_strategies import augmentation_with_duplication\n",
    "from maxsmi.augmentation_strategies import augmentation_without_duplication\n",
    "from maxsmi.augmentation_strategies import (\n",
    "    augmentation_with_reduced_duplication\n",
    ")\n",
    "from maxsmi.augmentation_strategies import augmentation_maximum_estimation\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def data_augmentation(dataSet,args):\n",
    "    smi_col = args['smiles_column']\n",
    "    task_names = args['task_names'][0]\n",
    "    df_empty = pd.DataFrame(columns=[smi_col,task_names])\n",
    "    for index,row in dataSet.iterrows():\n",
    "        smiles = row[smi_col]\n",
    "        non_duplicated_smiles = augmentation_with_reduced_duplication(smiles, 50)\n",
    "        for data in non_duplicated_smiles:\n",
    "            df = pd.DataFrame({smi_col:data,task_names:float(row[task_names])},index=[0])\n",
    "            df_empty = df_empty.append(df,ignore_index=True)\n",
    "    return df_empty\n",
    "\n",
    "def split_dataset_augmentation(my_df,seed,args):\n",
    "\n",
    "    training_data, data_test = train_test_split(my_df, test_size=0.1, random_state=seed)\n",
    "    data_train, data_val = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "    data_test = data_augmentation(data_test,args)\n",
    "    data_test = shuffle(data_test,random_state=seed) \n",
    "    data_train = data_augmentation(data_train,args)\n",
    "    data_train = shuffle(data_train,random_state=seed)\n",
    "    data_val = data_augmentation(data_val,args)\n",
    "    data_val = shuffle(data_val,random_state=seed)\n",
    "    test_set = load_dataset(args, data_test)\n",
    "    train_set = load_dataset(args, data_train)\n",
    "    val_set = load_dataset(args, data_val)\n",
    "       \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory result/CF-2274_NoAug_GAT_20220906 already exists.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda:'+ GPUNum)\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "if args['task_names'] is not None:\n",
    "    args['task_names'] = args['task_names'].split(',')\n",
    "\n",
    "args = init_featurizer(args)\n",
    "df = pd.read_csv(args['csv_path'])\n",
    "mkdir_p(args['result_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/2274\n",
      "Processing molecule 2000/2274\n"
     ]
    }
   ],
   "source": [
    "if args['augmentation']:\n",
    "    train_set, val_set, test_set = split_dataset_augmentation(df,seed,args)\n",
    "    args['n_tasks'] = train_set.n_tasks\n",
    "else:\n",
    "    dataset = load_dataset(args, df)\n",
    "    train_set, val_set, test_set = split_dataset(args, dataset,seed)\n",
    "    args['n_tasks'] = dataset.n_tasks\n",
    "    \n",
    "# Whether to take the logarithm of labels for narrowing the range of values\n",
    "if args['log_values']:\n",
    "    train_set.labels = train_set.labels.log()\n",
    "    val_set.labels = val_set.labels.log()\n",
    "    test_set.labels = test_set.labels.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['num_evals'] is not None:\n",
    "    assert args['num_evals'] > 0, 'Expect the number of hyperparameter search trials to ' \\\n",
    "                                  'be greater than 0, got {:d}'.format(args['num_evals'])\n",
    "    print('Start hyperparameter search with Bayesian '\n",
    "          'optimization for {:d} trials'.format(args['num_evals']))\n",
    "    trial_path = bayesian_optimization(args, train_set, val_set, test_set)\n",
    "else:\n",
    "    print('Use the manually specified hyperparameters')\n",
    "    exp_config = get_configure(args['model'])\n",
    "    main(args, exp_config, train_set, val_set, test_set)\n",
    "    trial_path = args['result_path'] + '/1'\n",
    "\n",
    "# Copy final\n",
    "copyfile(trial_path + '/model.pth', args['result_path'] + '/model.pth')\n",
    "copyfile(trial_path + '/configure.json', args['result_path'] + '/configure.json')\n",
    "copyfile(trial_path + '/eval.txt', args['result_path'] + '/eval.txt')\n",
    "\n",
    "with open(args['result_path']+'/configure.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyper file: result/CF-2274_NoAug_GAT_20220906/23\n"
     ]
    }
   ],
   "source": [
    "print('best hyper file: '+ trial_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.6741100115599353,\n",
       " 'atom_featurizer_type': 'canonical',\n",
       " 'batch_size': 32,\n",
       " 'bond_featurizer_type': 'canonical',\n",
       " 'dropout': 0.1750572819092699,\n",
       " 'gnn_hidden_feats': 128,\n",
       " 'in_node_feats': 74,\n",
       " 'lr': 0.002378781549117981,\n",
       " 'model': 'GAT',\n",
       " 'n_tasks': 1,\n",
       " 'num_gnn_layers': 1,\n",
       " 'num_heads': 8,\n",
       " 'patience': 30,\n",
       " 'predictor_hidden_feats': 128,\n",
       " 'residual': True,\n",
       " 'weight_decay': 0.0029333774037381562}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWithHyper (args, exp_config, train_set, val_set, test_set):\n",
    "\n",
    "    # Record settings\n",
    "    exp_config.update({\n",
    "        'model': args['model'],\n",
    "        'n_tasks': args['n_tasks'],\n",
    "        'atom_featurizer_type': args['atom_featurizer_type'],\n",
    "        'bond_featurizer_type': args['bond_featurizer_type'],\n",
    "#         'patience': patienceNum,\n",
    "#         'batch_size':batch_size\n",
    "    })\n",
    "    if args['atom_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "    if args['edge_featurizer'] is not None and args['bond_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "\n",
    "    # Set up directory for saving results\n",
    "#     args = init_trial_path(args)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=exp_config['batch_size'], shuffle=True,\n",
    "                              collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=exp_config['batch_size'],\n",
    "                            collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=exp_config['batch_size'],\n",
    "                             collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    model = load_model(exp_config).to(args['device'])\n",
    "    \n",
    "    best_model_file = args['result_path']+'/saved_model/%s_bst_%s.pth' % (args['model'], split)\n",
    "\n",
    "#     loss_criterion = nn.SmoothL1Loss(reduction='none')\n",
    "#     optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "#                      weight_decay=exp_config['weight_decay'])\n",
    "    \n",
    "#     stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "#                             filename=best_model_file,\n",
    "#                             metric=args['metric'])\n",
    "    loss_criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "    stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                            filename=best_model_file,\n",
    "                            metric=args['metric'])\n",
    "    for epoch in range(args['num_epochs']):\n",
    "#         # Train\n",
    "#         run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "#         # Validation and early stop\n",
    "#         val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "#         early_stop = stopper.step(val_score[args['metric']], model)\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "        # Validation and early stop\n",
    "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        early_stop = stopper.step(val_score['roc_auc_score'], model)\n",
    "\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    \n",
    "    tr_scores = run_an_eval_epoch(args, model, train_loader)\n",
    "    val_scores = run_an_eval_epoch(args, model, val_loader)\n",
    "    te_scores = run_an_eval_epoch(args, model, test_loader)\n",
    "    \n",
    "    return tr_scores,val_scores,te_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_res = []\n",
    "val_res = []\n",
    "te_res = []\n",
    "# for split in range(1, repetitions + 1):\n",
    "for split in range(1, repetitions + 1):\n",
    "    \n",
    "    if args['augmentation']:\n",
    "        train_set, val_set, test_set = split_dataset_augmentation(df, split,args)\n",
    "        args['n_tasks'] = train_set.n_tasks\n",
    "    else:\n",
    "        train_set, val_set, test_set = split_dataset(args,dataset,split)\n",
    "        args['n_tasks'] = dataset.n_tasks\n",
    "\n",
    "    # Whether to take the logarithm of labels for narrowing the range of values\n",
    "    if args['log_values']:\n",
    "        train_set.labels = train_set.labels.log()\n",
    "        val_set.labels = val_set.labels.log()\n",
    "        test_set.labels = test_set.labels.log()\n",
    "    print('n_tasks : '+ str(args['n_tasks']))\n",
    "\n",
    "    tr_scores,val_scores,te_scores = trainWithHyper(args, config, train_set, val_set, test_set)\n",
    "\n",
    "    tr_res.append(tr_scores);\n",
    "    val_res.append(val_scores);\n",
    "    te_res.append(te_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList(res):\n",
    "    auc_prc_list = []\n",
    "    acc_list = []\n",
    "    auc_roc_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    f1_list = []\n",
    "    kappa_list = []\n",
    "    mcc_list = []\n",
    "    for item in res:\n",
    "        auc_prc_list.append(item['all_score']['auc_prc'])\n",
    "        acc_list.append(item['all_score']['acc'])\n",
    "        auc_roc_list.append(item['all_score']['auc_roc'])\n",
    "        recall_list.append(item['all_score']['recall'])\n",
    "        precision_list.append(item['all_score']['precision'])\n",
    "        f1_list.append(item['all_score']['f1'])\n",
    "        kappa_list.append(item['all_score']['kappa'])\n",
    "        mcc_list.append(item['all_score']['mcc'])\n",
    "    return auc_prc_list,acc_list,auc_roc_list,recall_list,precision_list,f1_list,kappa_list,mcc_list\n",
    "\n",
    "tr_auc_prc_list,tr_acc_list,tr_auc_roc_list,tr_recall_list,tr_precision_list,tr_f1_list,tr_kappa_list,tr_mcc_list = getList(tr_res)\n",
    "val_auc_prc_list,val_acc_list,val_auc_roc_list,val_recall_list,val_precision_list,val_f1_list,val_kappa_list,val_mcc_list = getList(val_res)\n",
    "te_auc_prc_list,te_acc_list,te_auc_roc_list,te_recall_list,te_precision_list,te_f1_list,te_kappa_list,te_mcc_list = getList(te_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # acc auc_roc recall precision f1 kappa mcc\n",
    "    acc_str = 'acc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_acc_list), \n",
    "                    np.std(tr_acc_list),\n",
    "                    np.mean(val_acc_list), \n",
    "                    np.std(val_acc_list),\n",
    "                    np.mean(te_acc_list), \n",
    "                    np.std(te_acc_list),\n",
    "    )\n",
    "    auc_str = 'auc_roc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_auc_roc_list), \n",
    "                    np.std(tr_auc_roc_list),\n",
    "                    np.mean(val_auc_roc_list), \n",
    "                    np.std(val_auc_roc_list),\n",
    "                    np.mean(te_auc_roc_list), \n",
    "                    np.std(te_auc_roc_list),\n",
    "    )\n",
    "    recall_str = 'recall of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_recall_list), \n",
    "                    np.std(tr_recall_list),\n",
    "                    np.mean(val_recall_list), \n",
    "                    np.std(val_recall_list),\n",
    "                    np.mean(te_recall_list), \n",
    "                    np.std(te_recall_list),\n",
    "    )\n",
    "    precision_str = 'precision of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_precision_list), \n",
    "                    np.std(tr_precision_list),\n",
    "                    np.mean(val_precision_list), \n",
    "                    np.std(val_precision_list),\n",
    "                    np.mean(te_precision_list), \n",
    "                    np.std(te_precision_list),\n",
    "    )\n",
    "    f1_str = 'f1 of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_f1_list), \n",
    "                    np.std(tr_f1_list),\n",
    "                    np.mean(val_f1_list), \n",
    "                    np.std(val_f1_list),\n",
    "                    np.mean(te_f1_list), \n",
    "                    np.std(te_f1_list),\n",
    "    )\n",
    "    kappa_str = 'kappa of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_kappa_list), \n",
    "                    np.std(tr_kappa_list),\n",
    "                    np.mean(val_kappa_list), \n",
    "                    np.std(val_kappa_list),\n",
    "                    np.mean(te_kappa_list), \n",
    "                    np.std(te_kappa_list),\n",
    "    )\n",
    "    mcc_str = 'mcc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_mcc_list), \n",
    "                    np.std(tr_mcc_list),\n",
    "                    np.mean(val_mcc_list), \n",
    "                    np.std(val_mcc_list),\n",
    "                    np.mean(te_mcc_list), \n",
    "                    np.std(te_mcc_list),\n",
    "    )\n",
    "    auc_prc_str = 'auc_prc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                    np.mean(tr_auc_prc_list), \n",
    "                    np.std(tr_auc_prc_list),\n",
    "                    np.mean(val_auc_prc_list), \n",
    "                    np.std(val_auc_prc_list),\n",
    "                    np.mean(te_auc_prc_list), \n",
    "                    np.std(te_auc_prc_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of training set is 0.841±0.014, validation set is 0.841±0.025, test set is 0.834±0.028\n",
      "auc_roc of training set is 0.812±0.009, validation set is 0.821±0.035, test set is 0.803±0.036\n",
      "recall of training set is 0.215±0.145, validation set is 0.212±0.163, test set is 0.227±0.162\n",
      "precision of training set is 0.832±0.137, validation set is 0.806±0.210, test set is 0.767±0.220\n",
      "f1 of training set is 0.303±0.150, validation set is 0.293±0.161, test set is 0.305±0.165\n",
      "kappa of training set is 0.253±0.124, validation set is 0.244±0.138, test set is 0.252±0.141\n",
      "mcc of training set is 0.335±0.096, validation set is 0.326±0.117, test set is 0.323±0.125\n",
      "auc_prc of training set is 0.584±0.021, validation set is 0.588±0.070, test set is 0.571±0.077\n",
      "GAT\n"
     ]
    }
   ],
   "source": [
    "print(acc_str)\n",
    "print(auc_str)\n",
    "print(recall_str)\n",
    "print(precision_str)\n",
    "print(f1_str)\n",
    "print(kappa_str)\n",
    "print(mcc_str)\n",
    "print(auc_prc_str)\n",
    "print(args['model'])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args['result_path'] + '/output.txt', 'w') as f:\n",
    "    f.write(acc_str+'\\n')\n",
    "    f.write(auc_str+'\\n')\n",
    "    f.write(recall_str+'\\n')\n",
    "    f.write(precision_str+'\\n')\n",
    "    f.write(f1_str+'\\n')\n",
    "    f.write(kappa_str+'\\n')\n",
    "    f.write(mcc_str+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model: GAT</th>\n",
       "      <th>Train</th>\n",
       "      <th>Tr_STD</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Va_STD</th>\n",
       "      <th>Test</th>\n",
       "      <th>Te_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc</td>\n",
       "      <td>0.840990</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>0.840617</td>\n",
       "      <td>0.024712</td>\n",
       "      <td>0.833596</td>\n",
       "      <td>0.028443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc_roc</td>\n",
       "      <td>0.812105</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>0.820520</td>\n",
       "      <td>0.034533</td>\n",
       "      <td>0.802948</td>\n",
       "      <td>0.036275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.215069</td>\n",
       "      <td>0.145378</td>\n",
       "      <td>0.212326</td>\n",
       "      <td>0.162520</td>\n",
       "      <td>0.226736</td>\n",
       "      <td>0.161806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.832409</td>\n",
       "      <td>0.137392</td>\n",
       "      <td>0.805892</td>\n",
       "      <td>0.210109</td>\n",
       "      <td>0.767302</td>\n",
       "      <td>0.219669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.302609</td>\n",
       "      <td>0.149819</td>\n",
       "      <td>0.292673</td>\n",
       "      <td>0.161482</td>\n",
       "      <td>0.305009</td>\n",
       "      <td>0.165336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kappa</td>\n",
       "      <td>0.252837</td>\n",
       "      <td>0.123829</td>\n",
       "      <td>0.244438</td>\n",
       "      <td>0.138384</td>\n",
       "      <td>0.251659</td>\n",
       "      <td>0.140835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mcc</td>\n",
       "      <td>0.335137</td>\n",
       "      <td>0.096272</td>\n",
       "      <td>0.325791</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.322971</td>\n",
       "      <td>0.125435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>auc_prc</td>\n",
       "      <td>0.583633</td>\n",
       "      <td>0.021150</td>\n",
       "      <td>0.587654</td>\n",
       "      <td>0.070043</td>\n",
       "      <td>0.571404</td>\n",
       "      <td>0.076546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model: GAT     Train    Tr_STD  Validation    Va_STD      Test    Te_STD\n",
       "0        acc  0.840990  0.013607    0.840617  0.024712  0.833596  0.028443\n",
       "1    auc_roc  0.812105  0.009415    0.820520  0.034533  0.802948  0.036275\n",
       "2     recall  0.215069  0.145378    0.212326  0.162520  0.226736  0.161806\n",
       "3  precision  0.832409  0.137392    0.805892  0.210109  0.767302  0.219669\n",
       "4         f1  0.302609  0.149819    0.292673  0.161482  0.305009  0.165336\n",
       "5      kappa  0.252837  0.123829    0.244438  0.138384  0.251659  0.140835\n",
       "6        mcc  0.335137  0.096272    0.325791  0.117190  0.322971  0.125435\n",
       "7    auc_prc  0.583633  0.021150    0.587654  0.070043  0.571404  0.076546"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_acc_list,tr_auc_roc_list,tr_recall_list,tr_precision_list,tr_f1_list,tr_kappa_list,tr_mcc_list,tr_auc_prc_list\n",
    "val_acc_list,val_auc_roc_list,val_recall_list,val_precision_list,val_f1_list,val_kappa_list,val_mcc_list,tr_auc_prc_list\n",
    "te_acc_list,te_auc_roc_list,te_recall_list,te_precision_list,te_f1_list,te_kappa_list,te_mcc_list,tr_auc_prc_list\n",
    "\n",
    "import pandas as pd\n",
    "import collections\n",
    "dict1 = {\"model: \"+args['model']:['acc','auc_roc','recall','precision','f1','kappa','mcc','auc_prc'],\n",
    "         \"Train\":[np.mean(tr_acc_list),np.mean(tr_auc_roc_list),np.mean(tr_recall_list),np.mean(tr_precision_list), \n",
    "                  np.mean(tr_f1_list),np.mean(tr_kappa_list), np.mean(tr_mcc_list),np.mean(tr_auc_prc_list)],\n",
    "         \"Tr_STD\":[np.std(tr_acc_list),np.std(tr_auc_roc_list),np.std(tr_recall_list),np.std(tr_precision_list), \n",
    "                  np.std(tr_f1_list),np.std(tr_kappa_list), np.std(tr_mcc_list),np.std(tr_auc_prc_list)],\n",
    "         \"Validation\":[np.mean(val_acc_list),np.mean(val_auc_roc_list),np.mean(val_recall_list),np.mean(val_precision_list), \n",
    "                  np.mean(val_f1_list),np.mean(val_kappa_list), np.mean(val_mcc_list),np.mean(val_auc_prc_list)],\n",
    "         \"Va_STD\":[np.std(val_acc_list),np.std(val_auc_roc_list),np.std(val_recall_list),np.std(val_precision_list), \n",
    "                  np.std(val_f1_list),np.std(val_kappa_list), np.std(val_mcc_list),np.std(val_auc_prc_list)],\n",
    "         \"Test\":[np.mean(te_acc_list),np.mean(te_auc_roc_list),np.mean(te_recall_list),np.mean(te_precision_list), \n",
    "                  np.mean(te_f1_list),np.mean(te_kappa_list), np.mean(te_mcc_list),np.mean(te_auc_prc_list)],\n",
    "          \"Te_STD\":[np.std(te_acc_list),np.std(te_auc_roc_list),np.std(te_recall_list),np.std(te_precision_list), \n",
    "                  np.std(te_f1_list),np.std(te_kappa_list), np.std(te_mcc_list),np.std(te_auc_prc_list)]}\n",
    "dict1 = collections.OrderedDict(dict1)\n",
    "df = pd.DataFrame(dict1,index = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(args['result_path'] + '/output.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result/CF-2274_NoAug_GAT_20220906'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['result_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyper file: result/CF-2274_NoAug_GAT_20220906/23\n"
     ]
    }
   ],
   "source": [
    "print('best hyper file: '+ trial_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-dgllife]",
   "language": "python",
   "name": "conda-env-anaconda3-dgllife-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
